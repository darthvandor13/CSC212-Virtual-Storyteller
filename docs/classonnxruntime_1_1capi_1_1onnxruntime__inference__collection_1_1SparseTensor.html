<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: onnxruntime.capi.onnxruntime_inference_collection.SparseTensor Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceonnxruntime.html">onnxruntime</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1capi.html">capi</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1capi_1_1onnxruntime__inference__collection.html">onnxruntime_inference_collection</a></li><li class="navelem"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html">SparseTensor</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">onnxruntime.capi.onnxruntime_inference_collection.SparseTensor Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#acc4e6d91e6a92a3a9c9a6ce7d8ce06d3">__init__</a> (self, sparse_tensor)</td></tr>
<tr class="separator:acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2ef199248c42dc096e1188ff0a6c3a1"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a> (self)</td></tr>
<tr class="separator:aa2ef199248c42dc096e1188ff0a6c3a1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5525ae8dc1557d8f4289155e45f85076"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a5525ae8dc1557d8f4289155e45f85076">as_coo_view</a> (self)</td></tr>
<tr class="separator:a5525ae8dc1557d8f4289155e45f85076"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf5c56245e8b81dda5acbd37a2a0e27f"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#abf5c56245e8b81dda5acbd37a2a0e27f">as_csrc_view</a> (self)</td></tr>
<tr class="separator:abf5c56245e8b81dda5acbd37a2a0e27f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee109c888ce1a92de01da6897458b145"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aee109c888ce1a92de01da6897458b145">as_blocksparse_view</a> (self)</td></tr>
<tr class="separator:aee109c888ce1a92de01da6897458b145"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a27398cf0ef097b5d9c0381ecbaa170db"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a27398cf0ef097b5d9c0381ecbaa170db">to_cuda</a> (self, ort_device)</td></tr>
<tr class="separator:a27398cf0ef097b5d9c0381ecbaa170db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a84344947bcc7f9925f0f199028df37b6"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a84344947bcc7f9925f0f199028df37b6">format</a> (self)</td></tr>
<tr class="separator:a84344947bcc7f9925f0f199028df37b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af47ee66a49eba169c2f2d18c1f98bdd7"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a> (self)</td></tr>
<tr class="separator:af47ee66a49eba169c2f2d18c1f98bdd7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a45e7206d1cc2f8105516c5001ed92753"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a45e7206d1cc2f8105516c5001ed92753">data_type</a> (self)</td></tr>
<tr class="separator:a45e7206d1cc2f8105516c5001ed92753"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22b3324b8530b56c451af2e05d6747e8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a22b3324b8530b56c451af2e05d6747e8">device_name</a> (self)</td></tr>
<tr class="separator:a22b3324b8530b56c451af2e05d6747e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#acc4e6d91e6a92a3a9c9a6ce7d8ce06d3">__init__</a> (self, sparse_tensor)</td></tr>
<tr class="separator:acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2ef199248c42dc096e1188ff0a6c3a1"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a> (self)</td></tr>
<tr class="separator:aa2ef199248c42dc096e1188ff0a6c3a1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5525ae8dc1557d8f4289155e45f85076"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a5525ae8dc1557d8f4289155e45f85076">as_coo_view</a> (self)</td></tr>
<tr class="separator:a5525ae8dc1557d8f4289155e45f85076"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf5c56245e8b81dda5acbd37a2a0e27f"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#abf5c56245e8b81dda5acbd37a2a0e27f">as_csrc_view</a> (self)</td></tr>
<tr class="separator:abf5c56245e8b81dda5acbd37a2a0e27f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee109c888ce1a92de01da6897458b145"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aee109c888ce1a92de01da6897458b145">as_blocksparse_view</a> (self)</td></tr>
<tr class="separator:aee109c888ce1a92de01da6897458b145"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a27398cf0ef097b5d9c0381ecbaa170db"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a27398cf0ef097b5d9c0381ecbaa170db">to_cuda</a> (self, ort_device)</td></tr>
<tr class="separator:a27398cf0ef097b5d9c0381ecbaa170db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a84344947bcc7f9925f0f199028df37b6"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a84344947bcc7f9925f0f199028df37b6">format</a> (self)</td></tr>
<tr class="separator:a84344947bcc7f9925f0f199028df37b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af47ee66a49eba169c2f2d18c1f98bdd7"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a> (self)</td></tr>
<tr class="separator:af47ee66a49eba169c2f2d18c1f98bdd7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a45e7206d1cc2f8105516c5001ed92753"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a45e7206d1cc2f8105516c5001ed92753">data_type</a> (self)</td></tr>
<tr class="separator:a45e7206d1cc2f8105516c5001ed92753"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22b3324b8530b56c451af2e05d6747e8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a22b3324b8530b56c451af2e05d6747e8">device_name</a> (self)</td></tr>
<tr class="separator:a22b3324b8530b56c451af2e05d6747e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:a07c45e64bbc302bfaf65724597e58a33"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a07c45e64bbc302bfaf65724597e58a33">sparse_coo_from_numpy</a> (<a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a>, <a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a>, coo_indices, ort_device)</td></tr>
<tr class="separator:a07c45e64bbc302bfaf65724597e58a33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a2fb3eb503c82abcc5e259382bdaa29"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a1a2fb3eb503c82abcc5e259382bdaa29">sparse_csr_from_numpy</a> (<a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a>, <a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a>, inner_indices, outer_indices, ort_device)</td></tr>
<tr class="separator:a1a2fb3eb503c82abcc5e259382bdaa29"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07c45e64bbc302bfaf65724597e58a33"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a07c45e64bbc302bfaf65724597e58a33">sparse_coo_from_numpy</a> (<a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a>, <a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a>, coo_indices, ort_device)</td></tr>
<tr class="separator:a07c45e64bbc302bfaf65724597e58a33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a2fb3eb503c82abcc5e259382bdaa29"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#a1a2fb3eb503c82abcc5e259382bdaa29">sparse_csr_from_numpy</a> (<a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#af47ee66a49eba169c2f2d18c1f98bdd7">dense_shape</a>, <a class="el" href="classonnxruntime_1_1capi_1_1onnxruntime__inference__collection_1_1SparseTensor.html#aa2ef199248c42dc096e1188ff0a6c3a1">values</a>, inner_indices, outer_indices, ort_device)</td></tr>
<tr class="separator:a1a2fb3eb503c82abcc5e259382bdaa29"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">A data structure that project the C++ SparseTensor object
The class provides API to work with the object.
Depending on the format, the class will hold more than one buffer
depending on the format
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acc4e6d91e6a92a3a9c9a6ce7d8ce06d3">&#9670;&nbsp;</a></span>__init__() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sparse_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Internal constructor
</pre> 
</div>
</div>
<a id="acc4e6d91e6a92a3a9c9a6ce7d8ce06d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acc4e6d91e6a92a3a9c9a6ce7d8ce06d3">&#9670;&nbsp;</a></span>__init__() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sparse_tensor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Internal constructor
</pre> 
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aee109c888ce1a92de01da6897458b145"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aee109c888ce1a92de01da6897458b145">&#9670;&nbsp;</a></span>as_blocksparse_view() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_blocksparse_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return coo representation of the sparse tensor which will enable
querying BlockSparse indices. If the instance did not contain BlockSparse format, it would throw.
You can query coo indices as:

::

    block_sparse_indices = sparse_tensor.as_blocksparse_view().indices()

which will return a numpy array that is backed by the native memory
</pre> 
</div>
</div>
<a id="aee109c888ce1a92de01da6897458b145"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aee109c888ce1a92de01da6897458b145">&#9670;&nbsp;</a></span>as_blocksparse_view() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_blocksparse_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return coo representation of the sparse tensor which will enable
querying BlockSparse indices. If the instance did not contain BlockSparse format, it would throw.
You can query coo indices as:

::

    block_sparse_indices = sparse_tensor.as_blocksparse_view().indices()

which will return a numpy array that is backed by the native memory
</pre> 
</div>
</div>
<a id="a5525ae8dc1557d8f4289155e45f85076"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5525ae8dc1557d8f4289155e45f85076">&#9670;&nbsp;</a></span>as_coo_view() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_coo_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return coo representation of the sparse tensor which will enable
querying COO indices. If the instance did not contain COO format, it would throw.
You can query coo indices as:

::

    coo_indices = sparse_tensor.as_coo_view().indices()

which will return a numpy array that is backed by the native memory.
</pre> 
</div>
</div>
<a id="a5525ae8dc1557d8f4289155e45f85076"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5525ae8dc1557d8f4289155e45f85076">&#9670;&nbsp;</a></span>as_coo_view() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_coo_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return coo representation of the sparse tensor which will enable
querying COO indices. If the instance did not contain COO format, it would throw.
You can query coo indices as:

::

    coo_indices = sparse_tensor.as_coo_view().indices()

which will return a numpy array that is backed by the native memory.
</pre> 
</div>
</div>
<a id="abf5c56245e8b81dda5acbd37a2a0e27f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abf5c56245e8b81dda5acbd37a2a0e27f">&#9670;&nbsp;</a></span>as_csrc_view() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_csrc_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return CSR(C) representation of the sparse tensor which will enable
querying CRS(C) indices. If the instance dit not contain CSR(C) format, it would throw.
You can query indices as:

::

    inner_ndices = sparse_tensor.as_csrc_view().inner()
    outer_ndices = sparse_tensor.as_csrc_view().outer()

returning numpy arrays backed by the native memory.
</pre> 
</div>
</div>
<a id="abf5c56245e8b81dda5acbd37a2a0e27f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abf5c56245e8b81dda5acbd37a2a0e27f">&#9670;&nbsp;</a></span>as_csrc_view() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.as_csrc_view </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method will return CSR(C) representation of the sparse tensor which will enable
querying CRS(C) indices. If the instance dit not contain CSR(C) format, it would throw.
You can query indices as:

::

    inner_ndices = sparse_tensor.as_csrc_view().inner()
    outer_ndices = sparse_tensor.as_csrc_view().outer()

returning numpy arrays backed by the native memory.
</pre> 
</div>
</div>
<a id="a45e7206d1cc2f8105516c5001ed92753"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a45e7206d1cc2f8105516c5001ed92753">&#9670;&nbsp;</a></span>data_type() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.data_type </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a string data type of the data in the OrtValue
</pre> 
</div>
</div>
<a id="a45e7206d1cc2f8105516c5001ed92753"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a45e7206d1cc2f8105516c5001ed92753">&#9670;&nbsp;</a></span>data_type() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.data_type </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a string data type of the data in the OrtValue
</pre> 
</div>
</div>
<a id="af47ee66a49eba169c2f2d18c1f98bdd7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af47ee66a49eba169c2f2d18c1f98bdd7">&#9670;&nbsp;</a></span>dense_shape() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.dense_shape </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a numpy array(int64) containing a dense shape of a sparse tensor
</pre> 
</div>
</div>
<a id="af47ee66a49eba169c2f2d18c1f98bdd7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af47ee66a49eba169c2f2d18c1f98bdd7">&#9670;&nbsp;</a></span>dense_shape() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.dense_shape </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a numpy array(int64) containing a dense shape of a sparse tensor
</pre> 
</div>
</div>
<a id="a22b3324b8530b56c451af2e05d6747e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22b3324b8530b56c451af2e05d6747e8">&#9670;&nbsp;</a></span>device_name() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.device_name </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns the name of the device where the SparseTensor data buffers reside e.g. cpu, cuda
</pre> 
</div>
</div>
<a id="a22b3324b8530b56c451af2e05d6747e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a22b3324b8530b56c451af2e05d6747e8">&#9670;&nbsp;</a></span>device_name() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.device_name </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns the name of the device where the SparseTensor data buffers reside e.g. cpu, cuda
</pre> 
</div>
</div>
<a id="a84344947bcc7f9925f0f199028df37b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a84344947bcc7f9925f0f199028df37b6">&#9670;&nbsp;</a></span>format() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.format </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a OrtSparseFormat enumeration
</pre> 
</div>
</div>
<a id="a84344947bcc7f9925f0f199028df37b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a84344947bcc7f9925f0f199028df37b6">&#9670;&nbsp;</a></span>format() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.format </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a OrtSparseFormat enumeration
</pre> 
</div>
</div>
<a id="a07c45e64bbc302bfaf65724597e58a33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07c45e64bbc302bfaf65724597e58a33">&#9670;&nbsp;</a></span>sparse_coo_from_numpy() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.sparse_coo_from_numpy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dense_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coo_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Factory method to construct a SparseTensor in COO format from given arguments

:param dense_shape: 1-D  numpy array(int64) or a python list that contains a dense_shape of the sparse tensor
    must be on cpu memory
:param values: a homogeneous, contiguous 1-D numpy array that contains non-zero elements of the tensor
    of a type.
:param coo_indices:  contiguous numpy array(int64) that contains COO indices for the tensor. coo_indices may
    have a 1-D shape when it contains a linear index of non-zero values and its length must be equal to
    that of the values. It can also be of 2-D shape, in which has it contains pairs of coordinates for
    each of the nnz values and its length must be exactly twice of the values length.
:param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is
    suppored for non-numeric data types.

For primitive types, the method will map values and coo_indices arrays into native memory and will use
them as backing storage. It will increment the reference count for numpy arrays and it will decrement it
on GC. The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.
</pre> 
</div>
</div>
<a id="a07c45e64bbc302bfaf65724597e58a33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07c45e64bbc302bfaf65724597e58a33">&#9670;&nbsp;</a></span>sparse_coo_from_numpy() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.sparse_coo_from_numpy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dense_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coo_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Factory method to construct a SparseTensor in COO format from given arguments

:param dense_shape: 1-D  numpy array(int64) or a python list that contains a dense_shape of the sparse tensor
    must be on cpu memory
:param values: a homogeneous, contiguous 1-D numpy array that contains non-zero elements of the tensor
    of a type.
:param coo_indices:  contiguous numpy array(int64) that contains COO indices for the tensor. coo_indices may
    have a 1-D shape when it contains a linear index of non-zero values and its length must be equal to
    that of the values. It can also be of 2-D shape, in which has it contains pairs of coordinates for
    each of the nnz values and its length must be exactly twice of the values length.
:param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is
    suppored for non-numeric data types.

For primitive types, the method will map values and coo_indices arrays into native memory and will use
them as backing storage. It will increment the reference count for numpy arrays and it will decrement it
on GC. The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.
</pre> 
</div>
</div>
<a id="a1a2fb3eb503c82abcc5e259382bdaa29"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a2fb3eb503c82abcc5e259382bdaa29">&#9670;&nbsp;</a></span>sparse_csr_from_numpy() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.sparse_csr_from_numpy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dense_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>inner_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>outer_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Factory method to construct a SparseTensor in CSR format from given arguments

:param dense_shape: 1-D numpy array(int64) or a python list that contains a dense_shape of the
    sparse tensor (rows, cols) must be on cpu memory
:param values: a  contiguous, homogeneous 1-D numpy array that contains non-zero elements of the tensor
    of a type.
:param inner_indices:  contiguous 1-D numpy array(int64) that contains CSR inner indices for the tensor.
    Its length must be equal to that of the values.
:param outer_indices:  contiguous 1-D numpy array(int64) that contains CSR outer indices for the tensor.
    Its length must be equal to the number of rows + 1.
:param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is
    suppored for non-numeric data types.

For primitive types, the method will map values and indices arrays into native memory and will use them as
backing storage. It will increment the reference count and it will decrement then count when it is GCed.
The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.
</pre> 
</div>
</div>
<a id="a1a2fb3eb503c82abcc5e259382bdaa29"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a2fb3eb503c82abcc5e259382bdaa29">&#9670;&nbsp;</a></span>sparse_csr_from_numpy() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.sparse_csr_from_numpy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dense_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>inner_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>outer_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Factory method to construct a SparseTensor in CSR format from given arguments

:param dense_shape: 1-D numpy array(int64) or a python list that contains a dense_shape of the
    sparse tensor (rows, cols) must be on cpu memory
:param values: a  contiguous, homogeneous 1-D numpy array that contains non-zero elements of the tensor
    of a type.
:param inner_indices:  contiguous 1-D numpy array(int64) that contains CSR inner indices for the tensor.
    Its length must be equal to that of the values.
:param outer_indices:  contiguous 1-D numpy array(int64) that contains CSR outer indices for the tensor.
    Its length must be equal to the number of rows + 1.
:param ort_device: - describes the backing memory owned by the supplied nummpy arrays. Only CPU memory is
    suppored for non-numeric data types.

For primitive types, the method will map values and indices arrays into native memory and will use them as
backing storage. It will increment the reference count and it will decrement then count when it is GCed.
The buffers may reside in any storage either CPU or GPU.
For strings and objects, it will create a copy of the arrays in CPU memory as ORT does not support those
on other devices and their memory can not be mapped.
</pre> 
</div>
</div>
<a id="a27398cf0ef097b5d9c0381ecbaa170db"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a27398cf0ef097b5d9c0381ecbaa170db">&#9670;&nbsp;</a></span>to_cuda() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.to_cuda </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a copy of this instance on the specified cuda device

:param ort_device: with name 'cuda' and valid gpu device id

The method will throw if:

- this instance contains strings
- this instance is already on GPU. Cross GPU copy is not supported
- CUDA is not present in this build
- if the specified device is not valid
</pre> 
</div>
</div>
<a id="a27398cf0ef097b5d9c0381ecbaa170db"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a27398cf0ef097b5d9c0381ecbaa170db">&#9670;&nbsp;</a></span>to_cuda() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.to_cuda </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ort_device</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Returns a copy of this instance on the specified cuda device

:param ort_device: with name 'cuda' and valid gpu device id

The method will throw if:

- this instance contains strings
- this instance is already on GPU. Cross GPU copy is not supported
- CUDA is not present in this build
- if the specified device is not valid
</pre> 
</div>
</div>
<a id="aa2ef199248c42dc096e1188ff0a6c3a1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2ef199248c42dc096e1188ff0a6c3a1">&#9670;&nbsp;</a></span>values() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method returns a numpy array that is backed by the native memory
if the data type is numeric. Otherwise, the returned numpy array that contains
copies of the strings.
</pre> 
</div>
</div>
<a id="aa2ef199248c42dc096e1188ff0a6c3a1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2ef199248c42dc096e1188ff0a6c3a1">&#9670;&nbsp;</a></span>values() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.capi.onnxruntime_inference_collection.SparseTensor.values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The method returns a numpy array that is backed by the native memory
if the data type is numeric. Otherwise, the returned numpy array that contains
copies of the strings.
</pre> 
</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>chromadb_rest_wrapper/venv/lib/python3.10/site-packages/onnxruntime/capi/<a class="el" href="chromadb__rest__wrapper_2venv_2lib_2python3_810_2site-packages_2onnxruntime_2capi_2onnxruntime__inference__collection_8py.html">onnxruntime_inference_collection.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
