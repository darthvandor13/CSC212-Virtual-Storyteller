<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: onnxruntime.transformers.fusion_attention.FusionAttention Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceonnxruntime.html">onnxruntime</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1transformers.html">transformers</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1transformers_1_1fusion__attention.html">fusion_attention</a></li><li class="navelem"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html">FusionAttention</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">onnxruntime.transformers.fusion_attention.FusionAttention Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for onnxruntime.transformers.fusion_attention.FusionAttention:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention__inherit__graph.png" border="0" usemap="#aonnxruntime_8transformers_8fusion__attention_8FusionAttention_inherit__map" alt="Inheritance graph"/></div>
<!-- MAP 0 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for onnxruntime.transformers.fusion_attention.FusionAttention:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention__coll__graph.png" border="0" usemap="#aonnxruntime_8transformers_8fusion__attention_8FusionAttention_coll__map" alt="Collaboration graph"/></div>
<!-- MAP 1 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a20d8dfc00df201cc8435f9495bc9139b"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a20d8dfc00df201cc8435f9495bc9139b">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1AttentionMask.html">AttentionMask</a>|None <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a82c2fef1183bce91d537331a12b9e849">attention_mask</a>=None, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa0e766547b67c05989e0721a6f3cba19">use_multi_head_attention</a>=False, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad980899a21634fb5637f0f536491e489">disable_multi_head_attention_bias</a>=False, list[str] search_op_types=[&quot;SkipLayerNormalization&quot;, &quot;LayerNormalization&quot;])</td></tr>
<tr class="separator:a20d8dfc00df201cc8435f9495bc9139b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad956a5931ff18dfff09eaab2a53bfa16"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad956a5931ff18dfff09eaab2a53bfa16">get_num_heads_and_hidden_size_from_concat</a> (self, NodeProto concat)</td></tr>
<tr class="separator:ad956a5931ff18dfff09eaab2a53bfa16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a564a8301564a01596385d101ce5fe49f"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a564a8301564a01596385d101ce5fe49f">get_num_heads_and_hidden_size</a> (self, NodeProto reshape_q)</td></tr>
<tr class="separator:a564a8301564a01596385d101ce5fe49f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae59d05a8d684696f58b68cb5a3cafc52"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ae59d05a8d684696f58b68cb5a3cafc52">get_add_qk_str</a> (self, NodeProto add_qk)</td></tr>
<tr class="separator:ae59d05a8d684696f58b68cb5a3cafc52"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b01a3c135f30c4867a972bb6320cb9c"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a4b01a3c135f30c4867a972bb6320cb9c">reshape_add_qk</a> (self, str add_qk)</td></tr>
<tr class="separator:a4b01a3c135f30c4867a972bb6320cb9c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6120f96920cafe68cc73b4c8997d8fa"><td class="memItemLeft" align="right" valign="top">str&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa6120f96920cafe68cc73b4c8997d8fa">concat_kv</a> (self, str past_k, str past_v)</td></tr>
<tr class="separator:aa6120f96920cafe68cc73b4c8997d8fa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1cf8b10ef2464cdb209319c684c0b994"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a1cf8b10ef2464cdb209319c684c0b994">split_kv</a> (self, str present_k_name, str present_v_name, str kv_node)</td></tr>
<tr class="separator:a1cf8b10ef2464cdb209319c684c0b994"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aec73b7db7ad65877a2a4253a2ff5b575"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aec73b7db7ad65877a2a4253a2ff5b575">create_combined_qkv_bias</a> (self, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add, str name_prefix)</td></tr>
<tr class="separator:aec73b7db7ad65877a2a4253a2ff5b575"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab0b14c9c76b75e1e9978c32233b72d64"><td class="memItemLeft" align="right" valign="top">tuple[NodeProto, NodeProto, NodeProto]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ab0b14c9c76b75e1e9978c32233b72d64">create_packed_qkv_matmul_node</a> (self, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add)</td></tr>
<tr class="separator:ab0b14c9c76b75e1e9978c32233b72d64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeba4e2ce25af8b24ec0593a2a05e74c4"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aeba4e2ce25af8b24ec0593a2a05e74c4">create_multihead_attention_node</a> (self, NodeProto q_matmul, NodeProto|str|None k_matmul, NodeProto|str|None v_matmul, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, str output, str key_padding_mask=&quot;&quot;, str add_qk=&quot;&quot;, str past_k=&quot;&quot;, str past_v=&quot;&quot;, str present_k=&quot;&quot;, str present_v=&quot;&quot;, bool packed_qkv=False)</td></tr>
<tr class="separator:aeba4e2ce25af8b24ec0593a2a05e74c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d89d68a779189cf2b1b4a32753ebbbc"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a8d89d68a779189cf2b1b4a32753ebbbc">create_attention_node</a> (self, str|None mask_index, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, NodeProto q_add, NodeProto k_add, NodeProto v_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, str first_input, str output, str add_qk_str=&quot;&quot;, str past_k=&quot;&quot;, str past_v=&quot;&quot;, str present_k=&quot;&quot;, str present_v=&quot;&quot;, float|None scale=None, bool causal=False)</td></tr>
<tr class="separator:a8d89d68a779189cf2b1b4a32753ebbbc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa339520282f632ea91c58fd246ce144e"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa339520282f632ea91c58fd246ce144e">fuse</a> (self, node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:aa339520282f632ea91c58fd246ce144e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a20d8dfc00df201cc8435f9495bc9139b"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a20d8dfc00df201cc8435f9495bc9139b">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1AttentionMask.html">AttentionMask</a>|None <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a82c2fef1183bce91d537331a12b9e849">attention_mask</a>=None, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa0e766547b67c05989e0721a6f3cba19">use_multi_head_attention</a>=False, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad980899a21634fb5637f0f536491e489">disable_multi_head_attention_bias</a>=False, list[str] search_op_types=[&quot;SkipLayerNormalization&quot;, &quot;LayerNormalization&quot;])</td></tr>
<tr class="separator:a20d8dfc00df201cc8435f9495bc9139b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad956a5931ff18dfff09eaab2a53bfa16"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad956a5931ff18dfff09eaab2a53bfa16">get_num_heads_and_hidden_size_from_concat</a> (self, NodeProto concat)</td></tr>
<tr class="separator:ad956a5931ff18dfff09eaab2a53bfa16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a564a8301564a01596385d101ce5fe49f"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a564a8301564a01596385d101ce5fe49f">get_num_heads_and_hidden_size</a> (self, NodeProto reshape_q)</td></tr>
<tr class="separator:a564a8301564a01596385d101ce5fe49f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae59d05a8d684696f58b68cb5a3cafc52"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ae59d05a8d684696f58b68cb5a3cafc52">get_add_qk_str</a> (self, NodeProto add_qk)</td></tr>
<tr class="separator:ae59d05a8d684696f58b68cb5a3cafc52"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b01a3c135f30c4867a972bb6320cb9c"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a4b01a3c135f30c4867a972bb6320cb9c">reshape_add_qk</a> (self, str add_qk)</td></tr>
<tr class="separator:a4b01a3c135f30c4867a972bb6320cb9c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6120f96920cafe68cc73b4c8997d8fa"><td class="memItemLeft" align="right" valign="top">str&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa6120f96920cafe68cc73b4c8997d8fa">concat_kv</a> (self, str past_k, str past_v)</td></tr>
<tr class="separator:aa6120f96920cafe68cc73b4c8997d8fa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1cf8b10ef2464cdb209319c684c0b994"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a1cf8b10ef2464cdb209319c684c0b994">split_kv</a> (self, str present_k_name, str present_v_name, str kv_node)</td></tr>
<tr class="separator:a1cf8b10ef2464cdb209319c684c0b994"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aec73b7db7ad65877a2a4253a2ff5b575"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aec73b7db7ad65877a2a4253a2ff5b575">create_combined_qkv_bias</a> (self, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add, str name_prefix)</td></tr>
<tr class="separator:aec73b7db7ad65877a2a4253a2ff5b575"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab0b14c9c76b75e1e9978c32233b72d64"><td class="memItemLeft" align="right" valign="top">tuple[NodeProto, NodeProto, NodeProto]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ab0b14c9c76b75e1e9978c32233b72d64">create_packed_qkv_matmul_node</a> (self, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add)</td></tr>
<tr class="separator:ab0b14c9c76b75e1e9978c32233b72d64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeba4e2ce25af8b24ec0593a2a05e74c4"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aeba4e2ce25af8b24ec0593a2a05e74c4">create_multihead_attention_node</a> (self, NodeProto q_matmul, NodeProto|str|None k_matmul, NodeProto|str|None v_matmul, NodeProto q_add, NodeProto|None k_add, NodeProto|None v_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, str output, str key_padding_mask=&quot;&quot;, str add_qk=&quot;&quot;, str past_k=&quot;&quot;, str past_v=&quot;&quot;, str present_k=&quot;&quot;, str present_v=&quot;&quot;, bool packed_qkv=False)</td></tr>
<tr class="separator:aeba4e2ce25af8b24ec0593a2a05e74c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d89d68a779189cf2b1b4a32753ebbbc"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a8d89d68a779189cf2b1b4a32753ebbbc">create_attention_node</a> (self, str|None mask_index, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, NodeProto q_add, NodeProto k_add, NodeProto v_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a>, str first_input, str output, str add_qk_str=&quot;&quot;, str past_k=&quot;&quot;, str past_v=&quot;&quot;, str present_k=&quot;&quot;, str present_v=&quot;&quot;, float|None scale=None, bool causal=False)</td></tr>
<tr class="separator:a8d89d68a779189cf2b1b4a32753ebbbc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa339520282f632ea91c58fd246ce144e"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa339520282f632ea91c58fd246ce144e">fuse</a> (self, node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:aa339520282f632ea91c58fd246ce144e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html">onnxruntime.transformers.fusion_base.Fusion</a></td></tr>
<tr class="memitem:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a7d0d94dbf15fc205f4735e3d1e8828c5">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, str fused_op_type, str|list[str] search_op_types, str description=&quot;&quot;)</td></tr>
<tr class="separator:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab4ffc1eac3afcdfc42e471c9f2d41330">increase_counter</a> (self, str fused_op_name)</td></tr>
<tr class="separator:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab1bf9b45d1d3c486821b8f56fc9c3626">fuse</a> (self, NodeProto node, dict[str, list[NodeProto]] input_name_to_nodes, dict[str, NodeProto] output_name_to_node)</td></tr>
<tr class="separator:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a67b4a3359ea81d9c96c4fff4edec9fa8">apply</a> (self)</td></tr>
<tr class="separator:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a4f370334fa35b77d7667baf24ac346a1">add_initializer</a> (self, str name, int data_type, Sequence[int] dims, Any vals, bool raw=True)</td></tr>
<tr class="separator:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#aa4f74752db0beb76467a18f527234065">remove_initializer</a> (self, TensorProto tensor)</td></tr>
<tr class="separator:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a89db5ab3fec70db716d6ab2a8a30a650">add_nodes_to_remove</a> (self, list[NodeProto] nodes)</td></tr>
<tr class="separator:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a36a3534fd354c644d2b540b5e1e52e0e">add_nodes_to_remove_with_nodes_to_keep</a> (self, list[NodeProto] nodes, list[NodeProto] nodes_to_keep)</td></tr>
<tr class="separator:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a7d0d94dbf15fc205f4735e3d1e8828c5">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, str fused_op_type, str|list[str] search_op_types, str description=&quot;&quot;)</td></tr>
<tr class="separator:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab4ffc1eac3afcdfc42e471c9f2d41330">increase_counter</a> (self, str fused_op_name)</td></tr>
<tr class="separator:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab1bf9b45d1d3c486821b8f56fc9c3626">fuse</a> (self, NodeProto node, dict[str, list[NodeProto]] input_name_to_nodes, dict[str, NodeProto] output_name_to_node)</td></tr>
<tr class="separator:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a67b4a3359ea81d9c96c4fff4edec9fa8">apply</a> (self)</td></tr>
<tr class="separator:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a4f370334fa35b77d7667baf24ac346a1">add_initializer</a> (self, str name, int data_type, Sequence[int] dims, Any vals, bool raw=True)</td></tr>
<tr class="separator:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#aa4f74752db0beb76467a18f527234065">remove_initializer</a> (self, TensorProto tensor)</td></tr>
<tr class="separator:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a89db5ab3fec70db716d6ab2a8a30a650">add_nodes_to_remove</a> (self, list[NodeProto] nodes)</td></tr>
<tr class="separator:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a36a3534fd354c644d2b540b5e1e52e0e">add_nodes_to_remove_with_nodes_to_keep</a> (self, list[NodeProto] nodes, list[NodeProto] nodes_to_keep)</td></tr>
<tr class="separator:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a83017679409e727959f30fd85ac59a42"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a83017679409e727959f30fd85ac59a42">hidden_size</a></td></tr>
<tr class="separator:a83017679409e727959f30fd85ac59a42"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6de087f27811c6f4db17848f8d41a099"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a6de087f27811c6f4db17848f8d41a099">num_heads</a></td></tr>
<tr class="separator:a6de087f27811c6f4db17848f8d41a099"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a82c2fef1183bce91d537331a12b9e849"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a82c2fef1183bce91d537331a12b9e849">attention_mask</a></td></tr>
<tr class="separator:a82c2fef1183bce91d537331a12b9e849"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0e766547b67c05989e0721a6f3cba19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#aa0e766547b67c05989e0721a6f3cba19">use_multi_head_attention</a></td></tr>
<tr class="separator:aa0e766547b67c05989e0721a6f3cba19"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad980899a21634fb5637f0f536491e489"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad980899a21634fb5637f0f536491e489">disable_multi_head_attention_bias</a></td></tr>
<tr class="separator:ad980899a21634fb5637f0f536491e489"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a629298748ebf52302402c7bf8dfec0f0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a629298748ebf52302402c7bf8dfec0f0">mask_filter_value</a></td></tr>
<tr class="separator:a629298748ebf52302402c7bf8dfec0f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5007915f7d006427dec34b0f512f2cd2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a5007915f7d006427dec34b0f512f2cd2">num_heads_warning</a></td></tr>
<tr class="separator:a5007915f7d006427dec34b0f512f2cd2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad84a6e0eab1d075509b20c80cba735a8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#ad84a6e0eab1d075509b20c80cba735a8">hidden_size_warning</a></td></tr>
<tr class="separator:ad84a6e0eab1d075509b20c80cba735a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af02c88a6129c56aee638a8a429bf8571"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#af02c88a6129c56aee638a8a429bf8571">shape_infer</a></td></tr>
<tr class="separator:af02c88a6129c56aee638a8a429bf8571"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00dc575d2c938f4fc1004cc057ccf085"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a00dc575d2c938f4fc1004cc057ccf085">shape_infer_done</a></td></tr>
<tr class="separator:a00dc575d2c938f4fc1004cc057ccf085"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2057957e1abdc629d7f78a5f7ee6ac4c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1FusionAttention.html#a2057957e1abdc629d7f78a5f7ee6ac4c">prune_graph</a></td></tr>
<tr class="separator:a2057957e1abdc629d7f78a5f7ee6ac4c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion')"><img src="closed.png" alt="-"/>&#160;Public Attributes inherited from <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html">onnxruntime.transformers.fusion_base.Fusion</a></td></tr>
<tr class="memitem:a98ba45b80c0c61b8422469cedf9eee5e inherit pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a98ba45b80c0c61b8422469cedf9eee5e">this_graph_name</a></td></tr>
<tr class="separator:a98ba45b80c0c61b8422469cedf9eee5e inherit pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Fuse Attention subgraph into one Attention node.
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a20d8dfc00df201cc8435f9495bc9139b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a20d8dfc00df201cc8435f9495bc9139b">&#9670;&nbsp;</a></span>__init__() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a>&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1AttentionMask.html">AttentionMask</a> | None &#160;</td>
          <td class="paramname"><em>attention_mask</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>use_multi_head_attention</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>disable_multi_head_attention_bias</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str] &#160;</td>
          <td class="paramname"><em>search_op_types</em> = <code>[&quot;SkipLayerNormalization&quot;,&#160;&quot;LayerNormalization&quot;]</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a20d8dfc00df201cc8435f9495bc9139b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a20d8dfc00df201cc8435f9495bc9139b">&#9670;&nbsp;</a></span>__init__() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a>&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention_1_1AttentionMask.html">AttentionMask</a> | None &#160;</td>
          <td class="paramname"><em>attention_mask</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>use_multi_head_attention</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>disable_multi_head_attention_bias</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str] &#160;</td>
          <td class="paramname"><em>search_op_types</em> = <code>[&quot;SkipLayerNormalization&quot;,&#160;&quot;LayerNormalization&quot;]</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aa6120f96920cafe68cc73b4c8997d8fa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa6120f96920cafe68cc73b4c8997d8fa">&#9670;&nbsp;</a></span>concat_kv() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> str onnxruntime.transformers.fusion_attention.FusionAttention.concat_kv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>past_k</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>past_v</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Concatenate past_k and past_v inputs to create past_kv input.

Args:
    past_k (str): name of past K value
    past_v (str): name of past V value

Returns:
    kv_output_name (str): name of past KV value
</pre> 
</div>
</div>
<a id="aa6120f96920cafe68cc73b4c8997d8fa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa6120f96920cafe68cc73b4c8997d8fa">&#9670;&nbsp;</a></span>concat_kv() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> str onnxruntime.transformers.fusion_attention.FusionAttention.concat_kv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>past_k</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>past_v</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Concatenate past_k and past_v inputs to create past_kv input.

Args:
    past_k (str): name of past K value
    past_v (str): name of past V value

Returns:
    kv_output_name (str): name of past KV value
</pre> 
</div>
</div>
<a id="a8d89d68a779189cf2b1b4a32753ebbbc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d89d68a779189cf2b1b4a32753ebbbc">&#9670;&nbsp;</a></span>create_attention_node() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | None&#160;</td>
          <td class="paramname"><em>mask_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>first_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>add_qk_str</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | None &#160;</td>
          <td class="paramname"><em>scale</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>causal</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    mask_index (str | None): mask input
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    q_add (NodeProto): Add bias node in fully connection for Q
    k_add (NodeProto): Add bias node in fully connection for K
    v_add (NodeProto): Add bias node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    first_input (str): first input name
    output (str): output name
    add_qk_str (str): name of Add node after Q x K'
    past_k (str): name of input for past K value
    past_v (str): name of input for past V value
    present_k (str): name of output to store present K value
    present_v (str): name of output to store present V value
    scale: scale before softmax
    causal: whether it is uni-directional mask.

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="a8d89d68a779189cf2b1b4a32753ebbbc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d89d68a779189cf2b1b4a32753ebbbc">&#9670;&nbsp;</a></span>create_attention_node() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | None&#160;</td>
          <td class="paramname"><em>mask_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>first_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>add_qk_str</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | None &#160;</td>
          <td class="paramname"><em>scale</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>causal</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    mask_index (str | None): mask input
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    q_add (NodeProto): Add bias node in fully connection for Q
    k_add (NodeProto): Add bias node in fully connection for K
    v_add (NodeProto): Add bias node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    first_input (str): first input name
    output (str): output name
    add_qk_str (str): name of Add node after Q x K'
    past_k (str): name of input for past K value
    past_v (str): name of input for past V value
    present_k (str): name of output to store present K value
    present_v (str): name of output to store present V value
    scale: scale before softmax
    causal: whether it is uni-directional mask.

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="aec73b7db7ad65877a2a4253a2ff5b575"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aec73b7db7ad65877a2a4253a2ff5b575">&#9670;&nbsp;</a></span>create_combined_qkv_bias() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_combined_qkv_bias </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>name_prefix</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aec73b7db7ad65877a2a4253a2ff5b575"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aec73b7db7ad65877a2a4253a2ff5b575">&#9670;&nbsp;</a></span>create_combined_qkv_bias() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_combined_qkv_bias </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>name_prefix</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aeba4e2ce25af8b24ec0593a2a05e74c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeba4e2ce25af8b24ec0593a2a05e74c4">&#9670;&nbsp;</a></span>create_multihead_attention_node() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_multihead_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | str | None&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | str | None&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>key_padding_mask</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>add_qk</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>packed_qkv</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create a MultiHeadAttention node.

Args:
    q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
    k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
    v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
    q_add (NodeProto): name of Add from Q path
    k_add (NodeProto): name of Add from K path
    v_add (NodeProto): name of Add from V path
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    output (str): output name of MHA
    key_padding_mask (str): name of key padding mask
    add_qk (str): name of add after Q x K'
    past_k (str): name of past K value - (batch_size, num_heads, past_sequence_length, head_size)
    past_v (str): name of past V value - (batch_size, num_heads, past_sequence_length, head_size)
    present_k (str): name of present K value - (batch_size, num_heads, sequence_length, head_size)
    present_v (str): name of present V value - (batch_size, num_heads, sequence_length, head_size)
    packed_qkv (bool): whether to combine MatMuls from Q, K, V paths
                       Note: This is for the scenario where an Attention node should be created but cannot be created
                       because past_key and past_value are separate inputs and not one concatenated input.

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="aeba4e2ce25af8b24ec0593a2a05e74c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeba4e2ce25af8b24ec0593a2a05e74c4">&#9670;&nbsp;</a></span>create_multihead_attention_node() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention.FusionAttention.create_multihead_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | str | None&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | str | None&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>key_padding_mask</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>add_qk</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>past_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_k</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>present_v</em> = <code>&quot;&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>packed_qkv</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create a MultiHeadAttention node.

Args:
    q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
    k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
    v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size) or (batch_size, num_heads, past_sequence_length, head_size)
    q_add (NodeProto): name of Add from Q path
    k_add (NodeProto): name of Add from K path
    v_add (NodeProto): name of Add from V path
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    output (str): output name of MHA
    key_padding_mask (str): name of key padding mask
    add_qk (str): name of add after Q x K'
    past_k (str): name of past K value - (batch_size, num_heads, past_sequence_length, head_size)
    past_v (str): name of past V value - (batch_size, num_heads, past_sequence_length, head_size)
    present_k (str): name of present K value - (batch_size, num_heads, sequence_length, head_size)
    present_v (str): name of present V value - (batch_size, num_heads, sequence_length, head_size)
    packed_qkv (bool): whether to combine MatMuls from Q, K, V paths
                       Note: This is for the scenario where an Attention node should be created but cannot be created
                       because past_key and past_value are separate inputs and not one concatenated input.

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="ab0b14c9c76b75e1e9978c32233b72d64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0b14c9c76b75e1e9978c32233b72d64">&#9670;&nbsp;</a></span>create_packed_qkv_matmul_node() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[NodeProto, NodeProto, NodeProto] onnxruntime.transformers.fusion_attention.FusionAttention.create_packed_qkv_matmul_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create packed QKV MatMul node before MultiHeadAttention node.
   This is for the scenario where an Attention node should be created but cannot be created
   because past_key and past_value are separate inputs and not one concatenated input.

Args:
    q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
    k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size)
    v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size)
    q_add (NodeProto): name of Add from Q path
    k_add (NodeProto): name of Add from K path
    v_add (NodeProto): name of Add from V path

Returns:
     q_output (NodeProto): Slice node for Q
     k_output (NodeProto): Slice node for K
     v_output (NodeProto): Slice node for V
</pre> 
</div>
</div>
<a id="ab0b14c9c76b75e1e9978c32233b72d64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab0b14c9c76b75e1e9978c32233b72d64">&#9670;&nbsp;</a></span>create_packed_qkv_matmul_node() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[NodeProto, NodeProto, NodeProto] onnxruntime.transformers.fusion_attention.FusionAttention.create_packed_qkv_matmul_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>k_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto | None&#160;</td>
          <td class="paramname"><em>v_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create packed QKV MatMul node before MultiHeadAttention node.
   This is for the scenario where an Attention node should be created but cannot be created
   because past_key and past_value are separate inputs and not one concatenated input.

Args:
    q_matmul (NodeProto): name of MatMul from Q path - (batch_size, sequence_length, hidden_size)
    k_matmul (NodeProto): name of MatMul from K path - (batch_size, sequence_length, hidden_size)
    v_matmul (NodeProto): name of MatMul from V path - (batch_size, sequence_length, hidden_size)
    q_add (NodeProto): name of Add from Q path
    k_add (NodeProto): name of Add from K path
    v_add (NodeProto): name of Add from V path

Returns:
     q_output (NodeProto): Slice node for Q
     k_output (NodeProto): Slice node for K
     v_output (NodeProto): Slice node for V
</pre> 
</div>
</div>
<a id="aa339520282f632ea91c58fd246ce144e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa339520282f632ea91c58fd246ce144e">&#9670;&nbsp;</a></span>fuse() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.fuse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented in <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__tnlr_1_1FusionTnlrAttention.html#ae80348dee2057af8c7e29878d38a3ca8">onnxruntime.transformers.onnx_model_tnlr.FusionTnlrAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__t5_1_1FusionT5Attention.html#a8f39bb67804fdab39e1e05b23364d622">onnxruntime.transformers.onnx_model_t5.FusionT5Attention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__rotary__attention_1_1FusionRotaryAttention.html#a0bdc7c1a062911120ee33d6a0c5bbff9">onnxruntime.transformers.fusion_rotary_attention.FusionRotaryAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__conformer__attention_1_1FusionConformerAttention.html#a118ff16ca97751a1017f3d62504ea921">onnxruntime.transformers.fusion_conformer_attention.FusionConformerAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__bart__attention_1_1FusionBartAttention.html#a3c6f8fe1e4c69ec959c46f8b6a77b243">onnxruntime.transformers.fusion_bart_attention.FusionBartAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a9f2f704f23e6452640242d55747d312e">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__tnlr_1_1FusionTnlrAttention.html#ae80348dee2057af8c7e29878d38a3ca8">onnxruntime.transformers.onnx_model_tnlr.FusionTnlrAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__t5_1_1FusionT5Attention.html#a8f39bb67804fdab39e1e05b23364d622">onnxruntime.transformers.onnx_model_t5.FusionT5Attention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__rotary__attention_1_1FusionRotaryAttention.html#a0bdc7c1a062911120ee33d6a0c5bbff9">onnxruntime.transformers.fusion_rotary_attention.FusionRotaryAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__conformer__attention_1_1FusionConformerAttention.html#a118ff16ca97751a1017f3d62504ea921">onnxruntime.transformers.fusion_conformer_attention.FusionConformerAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__bart__attention_1_1FusionBartAttention.html#a3c6f8fe1e4c69ec959c46f8b6a77b243">onnxruntime.transformers.fusion_bart_attention.FusionBartAttention</a>, and <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a9f2f704f23e6452640242d55747d312e">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>.</p>

</div>
</div>
<a id="aa339520282f632ea91c58fd246ce144e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa339520282f632ea91c58fd246ce144e">&#9670;&nbsp;</a></span>fuse() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.fuse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented in <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__tnlr_1_1FusionTnlrAttention.html#ae80348dee2057af8c7e29878d38a3ca8">onnxruntime.transformers.onnx_model_tnlr.FusionTnlrAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__t5_1_1FusionT5Attention.html#a8f39bb67804fdab39e1e05b23364d622">onnxruntime.transformers.onnx_model_t5.FusionT5Attention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__rotary__attention_1_1FusionRotaryAttention.html#a0bdc7c1a062911120ee33d6a0c5bbff9">onnxruntime.transformers.fusion_rotary_attention.FusionRotaryAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__conformer__attention_1_1FusionConformerAttention.html#a118ff16ca97751a1017f3d62504ea921">onnxruntime.transformers.fusion_conformer_attention.FusionConformerAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__bart__attention_1_1FusionBartAttention.html#a3c6f8fe1e4c69ec959c46f8b6a77b243">onnxruntime.transformers.fusion_bart_attention.FusionBartAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a9f2f704f23e6452640242d55747d312e">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__tnlr_1_1FusionTnlrAttention.html#ae80348dee2057af8c7e29878d38a3ca8">onnxruntime.transformers.onnx_model_tnlr.FusionTnlrAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model__t5_1_1FusionT5Attention.html#a8f39bb67804fdab39e1e05b23364d622">onnxruntime.transformers.onnx_model_t5.FusionT5Attention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__rotary__attention_1_1FusionRotaryAttention.html#a0bdc7c1a062911120ee33d6a0c5bbff9">onnxruntime.transformers.fusion_rotary_attention.FusionRotaryAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__conformer__attention_1_1FusionConformerAttention.html#a118ff16ca97751a1017f3d62504ea921">onnxruntime.transformers.fusion_conformer_attention.FusionConformerAttention</a>, <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__bart__attention_1_1FusionBartAttention.html#a3c6f8fe1e4c69ec959c46f8b6a77b243">onnxruntime.transformers.fusion_bart_attention.FusionBartAttention</a>, and <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a9f2f704f23e6452640242d55747d312e">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>.</p>

</div>
</div>
<a id="ae59d05a8d684696f58b68cb5a3cafc52"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae59d05a8d684696f58b68cb5a3cafc52">&#9670;&nbsp;</a></span>get_add_qk_str() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.get_add_qk_str </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>add_qk</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ae59d05a8d684696f58b68cb5a3cafc52"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae59d05a8d684696f58b68cb5a3cafc52">&#9670;&nbsp;</a></span>get_add_qk_str() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.get_add_qk_str </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>add_qk</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a564a8301564a01596385d101ce5fe49f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a564a8301564a01596385d101ce5fe49f">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention.FusionAttention.get_num_heads_and_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size from a reshape node.

Args:
    reshape_q (NodeProto): reshape node for Q

Returns:
    Tuple[int, int]: num_heads and hidden_size
</pre> 
<p>Reimplemented in <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a86113789992585f498f299e776efa6d4">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>, and <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a86113789992585f498f299e776efa6d4">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>.</p>

</div>
</div>
<a id="a564a8301564a01596385d101ce5fe49f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a564a8301564a01596385d101ce5fe49f">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention.FusionAttention.get_num_heads_and_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size from a reshape node.

Args:
    reshape_q (NodeProto): reshape node for Q

Returns:
    Tuple[int, int]: num_heads and hidden_size
</pre> 
<p>Reimplemented in <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a86113789992585f498f299e776efa6d4">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>, and <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__clip_1_1FusionAttentionClip.html#a86113789992585f498f299e776efa6d4">onnxruntime.transformers.fusion_attention_clip.FusionAttentionClip</a>.</p>

</div>
</div>
<a id="ad956a5931ff18dfff09eaab2a53bfa16"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad956a5931ff18dfff09eaab2a53bfa16">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size_from_concat() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention.FusionAttention.get_num_heads_and_hidden_size_from_concat </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>concat</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size from Concat node in the following subgraph:

SkipLayerNormalization or EmbedLayerNormalization
                /        |
             MatMul    Shape
                |        |
               Add     Gather(indices=0)
                |        |
                |      Unsqueeze
                |        |
                |     Concat (*, -1, 12, 64)
                |     /
               Reshape
                  |
               Transpose
</pre> 
</div>
</div>
<a id="ad956a5931ff18dfff09eaab2a53bfa16"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad956a5931ff18dfff09eaab2a53bfa16">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size_from_concat() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention.FusionAttention.get_num_heads_and_hidden_size_from_concat </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>concat</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size from Concat node in the following subgraph:

SkipLayerNormalization or EmbedLayerNormalization
                /        |
             MatMul    Shape
                |        |
               Add     Gather(indices=0)
                |        |
                |      Unsqueeze
                |        |
                |     Concat (*, -1, 12, 64)
                |     /
               Reshape
                  |
               Transpose
</pre> 
</div>
</div>
<a id="a4b01a3c135f30c4867a972bb6320cb9c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b01a3c135f30c4867a972bb6320cb9c">&#9670;&nbsp;</a></span>reshape_add_qk() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.reshape_add_qk </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>add_qk</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4b01a3c135f30c4867a972bb6320cb9c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b01a3c135f30c4867a972bb6320cb9c">&#9670;&nbsp;</a></span>reshape_add_qk() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.reshape_add_qk </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>add_qk</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1cf8b10ef2464cdb209319c684c0b994"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1cf8b10ef2464cdb209319c684c0b994">&#9670;&nbsp;</a></span>split_kv() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.split_kv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>present_k_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>present_v_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>kv_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Split kv_node containing present KV values into separate present K and present V values.

Args:
    present_k_name (str): name of output to store present K value in
    present_v_name (str): name of output to store present V value in
    kv_node (str): name of present KV values
</pre> 
</div>
</div>
<a id="a1cf8b10ef2464cdb209319c684c0b994"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1cf8b10ef2464cdb209319c684c0b994">&#9670;&nbsp;</a></span>split_kv() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention.FusionAttention.split_kv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>present_k_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>present_v_name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>kv_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Split kv_node containing present KV values into separate present K and present V values.

Args:
    present_k_name (str): name of output to store present K value in
    present_v_name (str): name of output to store present V value in
    kv_node (str): name of present KV values
</pre> 
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a82c2fef1183bce91d537331a12b9e849"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a82c2fef1183bce91d537331a12b9e849">&#9670;&nbsp;</a></span>attention_mask</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.attention_mask</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad980899a21634fb5637f0f536491e489"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad980899a21634fb5637f0f536491e489">&#9670;&nbsp;</a></span>disable_multi_head_attention_bias</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.disable_multi_head_attention_bias</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a83017679409e727959f30fd85ac59a42"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83017679409e727959f30fd85ac59a42">&#9670;&nbsp;</a></span>hidden_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.hidden_size</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad84a6e0eab1d075509b20c80cba735a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad84a6e0eab1d075509b20c80cba735a8">&#9670;&nbsp;</a></span>hidden_size_warning</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.hidden_size_warning</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a629298748ebf52302402c7bf8dfec0f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a629298748ebf52302402c7bf8dfec0f0">&#9670;&nbsp;</a></span>mask_filter_value</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.mask_filter_value</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6de087f27811c6f4db17848f8d41a099"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6de087f27811c6f4db17848f8d41a099">&#9670;&nbsp;</a></span>num_heads</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.num_heads</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a5007915f7d006427dec34b0f512f2cd2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5007915f7d006427dec34b0f512f2cd2">&#9670;&nbsp;</a></span>num_heads_warning</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.num_heads_warning</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2057957e1abdc629d7f78a5f7ee6ac4c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2057957e1abdc629d7f78a5f7ee6ac4c">&#9670;&nbsp;</a></span>prune_graph</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.prune_graph</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="af02c88a6129c56aee638a8a429bf8571"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af02c88a6129c56aee638a8a429bf8571">&#9670;&nbsp;</a></span>shape_infer</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.shape_infer</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a00dc575d2c938f4fc1004cc057ccf085"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00dc575d2c938f4fc1004cc057ccf085">&#9670;&nbsp;</a></span>shape_infer_done</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.shape_infer_done</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aa0e766547b67c05989e0721a6f3cba19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0e766547b67c05989e0721a6f3cba19">&#9670;&nbsp;</a></span>use_multi_head_attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention.FusionAttention.use_multi_head_attention</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>chromadb_rest_wrapper/venv/lib/python3.10/site-packages/onnxruntime/transformers/<a class="el" href="chromadb__rest__wrapper_2venv_2lib_2python3_810_2site-packages_2onnxruntime_2transformers_2fusion__attention_8py.html">fusion_attention.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
