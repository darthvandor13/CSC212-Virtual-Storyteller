<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: langsmith.evaluation._runner Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacelangsmith.html">langsmith</a></li><li class="navelem"><a class="el" href="namespacelangsmith_1_1evaluation.html">evaluation</a></li><li class="navelem"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html">_runner</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">langsmith.evaluation._runner Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResultRow.html">ExperimentResultRow</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1__ExperimentManagerMixin.html">_ExperimentManagerMixin</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1__ExperimentManager.html">_ExperimentManager</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1__ForwardResults.html">_ForwardResults</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:aec3aab11a95c99c6d942ba3a21f4b4a7"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aec3aab11a95c99c6d942ba3a21f4b4a7">evaluate</a> (Union[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab4113c8825ee2565827ed474da561160">TARGET_T</a>, <a class="el" href="classlangchain__core_1_1runnables_1_1base_1_1Runnable.html">Runnable</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] target, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] data=None, Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>]] evaluators=None, Optional[Sequence[SUMMARY_EVALUATOR_T]] summary_evaluators=None, Optional[dict] metadata=None, Optional[str] experiment_prefix=None, Optional[str] description=None, Optional[int] max_concurrency=0, int num_repetitions=1, Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] client=None, bool blocking=True, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] experiment=None, bool upload_results=True, **Any kwargs)</td></tr>
<tr class="separator:aec3aab11a95c99c6d942ba3a21f4b4a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae417d08ef2e07a4a6a9993c4a7e5d29e"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ae417d08ef2e07a4a6a9993c4a7e5d29e">evaluate</a> (Union[Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]] target, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] data=None, Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>]] evaluators=None, Optional[Sequence[SUMMARY_EVALUATOR_T]] summary_evaluators=None, Optional[dict] metadata=None, Optional[str] experiment_prefix=None, Optional[str] description=None, Optional[int] max_concurrency=0, int num_repetitions=1, Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] client=None, bool blocking=True, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] experiment=None, bool upload_results=True, **Any kwargs)</td></tr>
<tr class="separator:ae417d08ef2e07a4a6a9993c4a7e5d29e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a541c70c8543adf6c16b9cfb8027510df"><td class="memItemLeft" align="right" valign="top">Union[<a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a>, <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a>]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a541c70c8543adf6c16b9cfb8027510df">evaluate</a> (Union[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab4113c8825ee2565827ed474da561160">TARGET_T</a>, <a class="el" href="classlangchain__core_1_1runnables_1_1base_1_1Runnable.html">Runnable</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]] target, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] data=None, Optional[Union[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>], Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>]]] evaluators=None, Optional[Sequence[SUMMARY_EVALUATOR_T]] summary_evaluators=None, Optional[dict] metadata=None, Optional[str] experiment_prefix=None, Optional[str] description=None, Optional[int] max_concurrency=0, int num_repetitions=1, Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] client=None, bool blocking=True, Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] experiment=None, bool upload_results=True, **Any kwargs)</td></tr>
<tr class="separator:a541c70c8543adf6c16b9cfb8027510df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26e61c2554d1be8078ffee1e2c0bab26"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a26e61c2554d1be8078ffee1e2c0bab26">evaluate_existing</a> (Union[str, uuid.UUID, <a class="el" href="classlangsmith_1_1schemas_1_1TracerSession.html">schemas.TracerSession</a>] experiment, Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>]] evaluators=None, Optional[Sequence[SUMMARY_EVALUATOR_T]] summary_evaluators=None, Optional[dict] metadata=None, Optional[int] max_concurrency=0, Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] client=None, bool load_nested=False, bool blocking=True)</td></tr>
<tr class="separator:a26e61c2554d1be8078ffee1e2c0bab26"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1aa92242e6bbd4f1130e2be37876c04c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a1aa92242e6bbd4f1130e2be37876c04c">evaluate_comparative</a> (Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] experiments, Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>] evaluators, Optional[str] experiment_prefix=None, Optional[str] description=None, int max_concurrency=5, Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] client=None, Optional[dict] metadata=None, bool load_nested=False, bool randomize_order=False)</td></tr>
<tr class="separator:a1aa92242e6bbd4f1130e2be37876c04c"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a30b1fe35f0a53551b52534ed6a150369"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a30b1fe35f0a53551b52534ed6a150369">DataFrame</a> = pd.DataFrame</td></tr>
<tr class="separator:a30b1fe35f0a53551b52534ed6a150369"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abb69e11a442f56b1a13733f94739852b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#abb69e11a442f56b1a13733f94739852b">logger</a> = logging.getLogger(__name__)</td></tr>
<tr class="separator:abb69e11a442f56b1a13733f94739852b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4113c8825ee2565827ed474da561160"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab4113c8825ee2565827ed474da561160">TARGET_T</a> = Union[Callable[[dict], dict], Callable[[dict, dict], dict]]</td></tr>
<tr class="separator:ab4113c8825ee2565827ed474da561160"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a95583b1467514610041306fb757e4ae4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a> = Union[str, uuid.UUID, Iterable[<a class="el" href="classlangsmith_1_1schemas_1_1Example.html">schemas.Example</a>], <a class="el" href="classlangsmith_1_1schemas_1_1Dataset.html">schemas.Dataset</a>]</td></tr>
<tr class="separator:a95583b1467514610041306fb757e4ae4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80d4847aaa84cf5c2817d8a9a997b9b7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a></td></tr>
<tr class="separator:a80d4847aaa84cf5c2817d8a9a997b9b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a51bf18c49d3178827c75d13d1a92106f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a51bf18c49d3178827c75d13d1a92106f">AEVALUATOR_T</a></td></tr>
<tr class="separator:a51bf18c49d3178827c75d13d1a92106f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab1d851700eca72667fbe5e8d530f2ef"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a> = Union[str, uuid.UUID, <a class="el" href="classlangsmith_1_1schemas_1_1TracerSession.html">schemas.TracerSession</a>]</td></tr>
<tr class="separator:aab1d851700eca72667fbe5e8d530f2ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab58b15820ef3294c384dd205030adfe3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a></td></tr>
<tr class="memdesc:ab58b15820ef3294c384dd205030adfe3"><td class="mdescLeft">&#160;</td><td class="mdescRight">Public API for Comparison Experiments.  <a href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">More...</a><br /></td></tr>
<tr class="separator:ab58b15820ef3294c384dd205030adfe3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a223797b0a19a159f33fcdcc12a8be7ce"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a223797b0a19a159f33fcdcc12a8be7ce">IT</a> = TypeVar(&quot;IT&quot;)</td></tr>
<tr class="separator:a223797b0a19a159f33fcdcc12a8be7ce"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad6de50c4c105251c50847d0602a8516"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aad6de50c4c105251c50847d0602a8516">ET</a> = TypeVar(&quot;ET&quot;, bound=&quot;_ExperimentManagerMixin&quot;)</td></tr>
<tr class="separator:aad6de50c4c105251c50847d0602a8516"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">V2 Evaluation Interface.</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a541c70c8543adf6c16b9cfb8027510df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a541c70c8543adf6c16b9cfb8027510df">&#9670;&nbsp;</a></span>evaluate() <span class="overload">[1/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[<a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a>, <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a>] langsmith.evaluation._runner.evaluate </td>
          <td>(</td>
          <td class="paramtype">Union[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab4113c8825ee2565827ed474da561160">TARGET_T</a>, <a class="el" href="classlangchain__core_1_1runnables_1_1base_1_1Runnable.html">Runnable</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]]&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] &#160;</td>
          <td class="paramname"><em>data</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[
        Union[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>], Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>]]
    ] &#160;</td>
          <td class="paramname"><em>evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[SUMMARY_EVALUATOR_T]] &#160;</td>
          <td class="paramname"><em>summary_evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[dict] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>experiment_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>description</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>max_concurrency</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_repetitions</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] &#160;</td>
          <td class="paramname"><em>client</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>blocking</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] &#160;</td>
          <td class="paramname"><em>experiment</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>upload_results</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Evaluate a target system on a given dataset.

Args:
    target (TARGET_T | Runnable | EXPERIMENT_T | Tuple[EXPERIMENT_T, EXPERIMENT_T]):
        The target system or experiment(s) to evaluate. Can be a function
        that takes a dict and returns a dict, a langchain Runnable, an
        existing experiment ID, or a two-tuple of experiment IDs.
    data (DATA_T): The dataset to evaluate on. Can be a dataset name, a list of
        examples, or a generator of examples.
    evaluators (Sequence[EVALUATOR_T] | Sequence[COMPARATIVE_EVALUATOR_T] | None):
        A list of evaluators to run on each example. The evaluator signature
        depends on the target type. Default to None.
    summary_evaluators (Sequence[SUMMARY_EVALUATOR_T] | None): A list of summary
        evaluators to run on the entire dataset. Should not be specified if
        comparing two existing experiments. Defaults to None.
    metadata (dict | None): Metadata to attach to the experiment.
        Defaults to None.
    experiment_prefix (str | None): A prefix to provide for your experiment name.
        Defaults to None.
    description (str | None): A free-form text description for the experiment.
    max_concurrency (int | None): The maximum number of concurrent
        evaluations to run. If None then no limit is set. If 0 then no concurrency.
        Defaults to 0.
    client (langsmith.Client | None): The LangSmith client to use.
        Defaults to None.
    blocking (bool): Whether to block until the evaluation is complete.
        Defaults to True.
    num_repetitions (int): The number of times to run the evaluation.
        Each item in the dataset will be run and evaluated this many times.
        Defaults to 1.
    experiment (schemas.TracerSession | None): An existing experiment to
        extend. If provided, experiment_prefix is ignored. For advanced
        usage only. Should not be specified if target is an existing experiment or
        two-tuple fo experiments.
    load_nested (bool): Whether to load all child runs for the experiment.
        Default is to only load the top-level root runs. Should only be specified
        when target is an existing experiment or two-tuple of experiments.
    randomize_order (bool): Whether to randomize the order of the outputs for each
        evaluation. Default is False. Should only be specified when target is a
        two-tuple of existing experiments.

Returns:
    ExperimentResults: If target is a function, Runnable, or existing experiment.
    ComparativeExperimentResults: If target is a two-tuple of existing experiments.

Examples:
    Prepare the dataset:

    &gt;&gt;&gt; from typing import Sequence
    &gt;&gt;&gt; from langsmith import Client
    &gt;&gt;&gt; from langsmith.evaluation import evaluate
    &gt;&gt;&gt; from langsmith.schemas import Example, Run
    &gt;&gt;&gt; client = Client()
    &gt;&gt;&gt; dataset = client.clone_public_dataset(
    ...     "https://smith.langchain.com/public/419dcab2-1d66-4b94-8901-0357ead390df/d"
    ... )
    &gt;&gt;&gt; dataset_name = "Evaluate Examples"

    Basic usage:

    &gt;&gt;&gt; def accuracy(run: Run, example: Example):
    ...     # Row-level evaluator for accuracy.
    ...     pred = run.outputs["output"]
    ...     expected = example.outputs["answer"]
    ...     return {"score": expected.lower() == pred.lower()}
    &gt;&gt;&gt; def precision(runs: Sequence[Run], examples: Sequence[Example]):
    ...     # Experiment-level evaluator for precision.
    ...     # TP / (TP + FP)
    ...     predictions = [run.outputs["output"].lower() for run in runs]
    ...     expected = [example.outputs["answer"].lower() for example in examples]
    ...     # yes and no are the only possible answers
    ...     tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    ...     fp = sum([p == "yes" and e == "no" for p, e in zip(predictions, expected)])
    ...     return {"score": tp / (tp + fp)}
    &gt;&gt;&gt; def predict(inputs: dict) -&gt; dict:
    ...     # This can be any function or just an API call to your app.
    ...     return {"output": "Yes"}
    &gt;&gt;&gt; results = evaluate(
    ...     predict,
    ...     data=dataset_name,
    ...     evaluators=[accuracy],
    ...     summary_evaluators=[precision],
    ...     experiment_prefix="My Experiment",
    ...     description="Evaluating the accuracy of a simple prediction model.",
    ...     metadata={
    ...         "my-prompt-version": "abcd-1234",
    ...     },
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...

    Evaluating over only a subset of the examples

    &gt;&gt;&gt; experiment_name = results.experiment_name
    &gt;&gt;&gt; examples = client.list_examples(dataset_name=dataset_name, limit=5)
    &gt;&gt;&gt; results = evaluate(
    ...     predict,
    ...     data=examples,
    ...     evaluators=[accuracy],
    ...     summary_evaluators=[precision],
    ...     experiment_prefix="My Experiment",
    ...     description="Just testing a subset synchronously.",
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...

    Streaming each prediction to more easily + eagerly debug.

    &gt;&gt;&gt; results = evaluate(
    ...     predict,
    ...     data=dataset_name,
    ...     evaluators=[accuracy],
    ...     summary_evaluators=[precision],
    ...     description="I don't even have to block!",
    ...     blocking=False,
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...
    &gt;&gt;&gt; for i, result in enumerate(results):  # doctest: +ELLIPSIS
    ...     pass

    Using the `evaluate` API with an off-the-shelf LangChain evaluator:

    &gt;&gt;&gt; from langsmith.evaluation import LangChainStringEvaluator
    &gt;&gt;&gt; from langchain_openai import ChatOpenAI
    &gt;&gt;&gt; def prepare_criteria_data(run: Run, example: Example):
    ...     return {
    ...         "prediction": run.outputs["output"],
    ...         "reference": example.outputs["answer"],
    ...         "input": str(example.inputs),
    ...     }
    &gt;&gt;&gt; results = evaluate(
    ...     predict,
    ...     data=dataset_name,
    ...     evaluators=[
    ...         accuracy,
    ...         LangChainStringEvaluator("embedding_distance"),
    ...         LangChainStringEvaluator(
    ...             "labeled_criteria",
    ...             config={
    ...                 "criteria": {
    ...                     "usefulness": "The prediction is useful if it is correct"
    ...                     " and/or asks a useful followup question."
    ...                 },
    ...                 "llm": ChatOpenAI(model="gpt-4o"),
    ...             },
    ...             prepare_data=prepare_criteria_data,
    ...         ),
    ...     ],
    ...     description="Evaluating with off-the-shelf LangChain evaluators.",
    ...     summary_evaluators=[precision],
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...

    Evaluating a LangChain object:

    &gt;&gt;&gt; from langchain_core.runnables import chain as as_runnable
    &gt;&gt;&gt; @as_runnable
    ... def nested_predict(inputs):
    ...     return {"output": "Yes"}
    &gt;&gt;&gt; @as_runnable
    ... def lc_predict(inputs):
    ...     return nested_predict.invoke(inputs)
    &gt;&gt;&gt; results = evaluate(
    ...     lc_predict.invoke,
    ...     data=dataset_name,
    ...     evaluators=[accuracy],
    ...     description="This time we're evaluating a LangChain object.",
    ...     summary_evaluators=[precision],
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...

.. versionchanged:: 0.2.0

    'max_concurrency' default updated from None (no limit on concurrency)
    to 0 (no concurrency at all).
</pre> 
</div>
</div>
<a id="aec3aab11a95c99c6d942ba3a21f4b4a7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aec3aab11a95c99c6d942ba3a21f4b4a7">&#9670;&nbsp;</a></span>evaluate() <span class="overload">[2/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a> langsmith.evaluation._runner.evaluate </td>
          <td>(</td>
          <td class="paramtype">Union[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab4113c8825ee2565827ed474da561160">TARGET_T</a>, <a class="el" href="classlangchain__core_1_1runnables_1_1base_1_1Runnable.html">Runnable</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] &#160;</td>
          <td class="paramname"><em>data</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>]] &#160;</td>
          <td class="paramname"><em>evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[SUMMARY_EVALUATOR_T]] &#160;</td>
          <td class="paramname"><em>summary_evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[dict] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>experiment_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>description</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>max_concurrency</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_repetitions</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] &#160;</td>
          <td class="paramname"><em>client</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>blocking</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] &#160;</td>
          <td class="paramname"><em>experiment</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>upload_results</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ae417d08ef2e07a4a6a9993c4a7e5d29e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae417d08ef2e07a4a6a9993c4a7e5d29e">&#9670;&nbsp;</a></span>evaluate() <span class="overload">[3/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a> langsmith.evaluation._runner.evaluate </td>
          <td>(</td>
          <td class="paramtype">Union[Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]]&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a95583b1467514610041306fb757e4ae4">DATA_T</a>] &#160;</td>
          <td class="paramname"><em>data</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>]] &#160;</td>
          <td class="paramname"><em>evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[SUMMARY_EVALUATOR_T]] &#160;</td>
          <td class="paramname"><em>summary_evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[dict] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>experiment_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>description</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>max_concurrency</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>num_repetitions</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] &#160;</td>
          <td class="paramname"><em>client</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>blocking</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>] &#160;</td>
          <td class="paramname"><em>experiment</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>upload_results</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1aa92242e6bbd4f1130e2be37876c04c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1aa92242e6bbd4f1130e2be37876c04c">&#9670;&nbsp;</a></span>evaluate_comparative()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ComparativeExperimentResults.html">ComparativeExperimentResults</a> langsmith.evaluation._runner.evaluate_comparative </td>
          <td>(</td>
          <td class="paramtype">Tuple[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>, <a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#aab1d851700eca72667fbe5e8d530f2ef">EXPERIMENT_T</a>]&#160;</td>
          <td class="paramname"><em>experiments</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#ab58b15820ef3294c384dd205030adfe3">COMPARATIVE_EVALUATOR_T</a>]&#160;</td>
          <td class="paramname"><em>evaluators</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>experiment_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>description</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>max_concurrency</em> = <code>5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] &#160;</td>
          <td class="paramname"><em>client</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[dict] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>load_nested</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>randomize_order</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Evaluate existing experiment runs against each other.

This lets you use pairwise preference scoring to generate more
reliable feedback in your experiments.

Args:
    experiments (Tuple[Union[str, uuid.UUID], Union[str, uuid.UUID]]):
        The identifiers of the experiments to compare.
    evaluators (Sequence[COMPARATIVE_EVALUATOR_T]):
        A list of evaluators to run on each example.
    experiment_prefix (Optional[str]): A prefix to provide for your experiment name.
        Defaults to None.
    description (Optional[str]): A free-form text description for the experiment.
    max_concurrency (int): The maximum number of concurrent evaluations to run.
        Defaults to 5.
    client (Optional[langsmith.Client]): The LangSmith client to use.
        Defaults to None.
    metadata (Optional[dict]): Metadata to attach to the experiment.
        Defaults to None.
    load_nested (bool): Whether to load all child runs for the experiment.
        Default is to only load the top-level root runs.
    randomize_order (bool): Whether to randomize the order of the outputs for each evaluation.
        Default is False.

Returns:
    ComparativeExperimentResults: The results of the comparative evaluation.

Examples:
    Suppose you want to compare two prompts to see which one is more effective.
    You would first prepare your dataset:

    &gt;&gt;&gt; from typing import Sequence
    &gt;&gt;&gt; from langsmith import Client
    &gt;&gt;&gt; from langsmith.evaluation import evaluate
    &gt;&gt;&gt; from langsmith.schemas import Example, Run
    &gt;&gt;&gt; client = Client()
    &gt;&gt;&gt; dataset = client.clone_public_dataset(
    ...     "https://smith.langchain.com/public/419dcab2-1d66-4b94-8901-0357ead390df/d"
    ... )
    &gt;&gt;&gt; dataset_name = "Evaluate Examples"

    Then you would run your different prompts:
    &gt;&gt;&gt; import functools
    &gt;&gt;&gt; import openai
    &gt;&gt;&gt; from langsmith.evaluation import evaluate
    &gt;&gt;&gt; from langsmith.wrappers import wrap_openai
    &gt;&gt;&gt; oai_client = openai.Client()
    &gt;&gt;&gt; wrapped_client = wrap_openai(oai_client)
    &gt;&gt;&gt; prompt_1 = "You are a helpful assistant."
    &gt;&gt;&gt; prompt_2 = "You are an exceedingly helpful assistant."
    &gt;&gt;&gt; def predict(inputs: dict, prompt: str) -&gt; dict:
    ...     completion = wrapped_client.chat.completions.create(
    ...         model="gpt-4o-mini",
    ...         messages=[
    ...             {"role": "system", "content": prompt},
    ...             {
    ...                 "role": "user",
    ...                 "content": f"Context: {inputs['context']}"
    ...                 f"\n\ninputs['question']",
    ...             },
    ...         ],
    ...     )
    ...     return {"output": completion.choices[0].message.content}
    &gt;&gt;&gt; results_1 = evaluate(
    ...     functools.partial(predict, prompt=prompt_1),
    ...     data=dataset_name,
    ...     description="Evaluating our basic system prompt.",
    ...     blocking=False,  # Run these experiments in parallel
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...
    &gt;&gt;&gt; results_2 = evaluate(
    ...     functools.partial(predict, prompt=prompt_2),
    ...     data=dataset_name,
    ...     description="Evaluating our advanced system prompt.",
    ...     blocking=False,
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...
    &gt;&gt;&gt; results_1.wait()
    &gt;&gt;&gt; results_2.wait()

        Finally, you would compare the two prompts directly:
    &gt;&gt;&gt; import json
    &gt;&gt;&gt; from langsmith.evaluation import evaluate_comparative
    &gt;&gt;&gt; from langsmith import schemas
    &gt;&gt;&gt; def score_preferences(runs: list, example: schemas.Example):
    ...     assert len(runs) == 2  # Comparing 2 systems
    ...     assert isinstance(example, schemas.Example)
    ...     assert all(run.reference_example_id == example.id for run in runs)
    ...     pred_a = runs[0].outputs["output"]
    ...     pred_b = runs[1].outputs["output"]
    ...     ground_truth = example.outputs["answer"]
    ...     tools = [
    ...         {
    ...             "type": "function",
    ...             "function": {
    ...                 "name": "rank_preferences",
    ...                 "description": "Saves the prefered response ('A' or 'B')",
    ...                 "parameters": {
    ...                     "type": "object",
    ...                     "properties": {
    ...                         "reasoning": {
    ...                             "type": "string",
    ...                             "description": "The reasoning behind the choice.",
    ...                         },
    ...                         "preferred_option": {
    ...                             "type": "string",
    ...                             "enum": ["A", "B"],
    ...                             "description": "The preferred option, either 'A' or 'B'",
    ...                         },
    ...                     },
    ...                     "required": ["preferred_option"],
    ...                 },
    ...             },
    ...         }
    ...     ]
    ...     completion = openai.Client().chat.completions.create(
    ...         model="gpt-4o-mini",
    ...         messages=[
    ...             {"role": "system", "content": "Select the better response."},
    ...             {
    ...                 "role": "user",
    ...                 "content": f"Option A: {pred_a}"
    ...                 f"\n\nOption B: {pred_b}"
    ...                 f"\n\nGround Truth: {ground_truth}",
    ...             },
    ...         ],
    ...         tools=tools,
    ...         tool_choice={
    ...             "type": "function",
    ...             "function": {"name": "rank_preferences"},
    ...         },
    ...     )
    ...     tool_args = completion.choices[0].message.tool_calls[0].function.arguments
    ...     loaded_args = json.loads(tool_args)
    ...     preference = loaded_args["preferred_option"]
    ...     comment = loaded_args["reasoning"]
    ...     if preference == "A":
    ...         return {
    ...             "key": "ranked_preference",
    ...             "scores": {runs[0].id: 1, runs[1].id: 0},
    ...             "comment": comment,
    ...         }
    ...     else:
    ...         return {
    ...             "key": "ranked_preference",
    ...             "scores": {runs[0].id: 0, runs[1].id: 1},
    ...             "comment": comment,
    ...         }
    &gt;&gt;&gt; def score_length_difference(runs: list, example: schemas.Example):
    ...     # Just return whichever response is longer.
    ...     # Just an example, not actually useful in real life.
    ...     assert len(runs) == 2  # Comparing 2 systems
    ...     assert isinstance(example, schemas.Example)
    ...     assert all(run.reference_example_id == example.id for run in runs)
    ...     pred_a = runs[0].outputs["output"]
    ...     pred_b = runs[1].outputs["output"]
    ...     if len(pred_a) &gt; len(pred_b):
    ...         return {
    ...             "key": "length_difference",
    ...             "scores": {runs[0].id: 1, runs[1].id: 0},
    ...         }
    ...     else:
    ...         return {
    ...             "key": "length_difference",
    ...             "scores": {runs[0].id: 0, runs[1].id: 1},
    ...         }
    &gt;&gt;&gt; results = evaluate_comparative(
    ...     [results_1.experiment_name, results_2.experiment_name],
    ...     evaluators=[score_preferences, score_length_difference],
    ...     client=client,
    ... )  # doctest: +ELLIPSIS
    View the pairwise evaluation results at:...
    &gt;&gt;&gt; eval_results = list(results)
    &gt;&gt;&gt; assert len(eval_results) &gt;= 10  # doctest: +SKIP
    &gt;&gt;&gt; assert all(
    ...     "feedback.ranked_preference" in r["evaluation_results"]
    ...     for r in eval_results
    ... )  # doctest: +SKIP
    &gt;&gt;&gt; assert all(
    ...     "feedback.length_difference" in r["evaluation_results"]
    ...     for r in eval_results
    ... )  # doctest: +SKIP
</pre> 
</div>
</div>
<a id="a26e61c2554d1be8078ffee1e2c0bab26"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26e61c2554d1be8078ffee1e2c0bab26">&#9670;&nbsp;</a></span>evaluate_existing()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangsmith_1_1evaluation_1_1__runner_1_1ExperimentResults.html">ExperimentResults</a> langsmith.evaluation._runner.evaluate_existing </td>
          <td>(</td>
          <td class="paramtype">Union[str, uuid.UUID, <a class="el" href="classlangsmith_1_1schemas_1_1TracerSession.html">schemas.TracerSession</a>]&#160;</td>
          <td class="paramname"><em>experiment</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[<a class="el" href="namespacelangsmith_1_1evaluation_1_1__runner.html#a80d4847aaa84cf5c2817d8a9a997b9b7">EVALUATOR_T</a>]] &#160;</td>
          <td class="paramname"><em>evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Sequence[SUMMARY_EVALUATOR_T]] &#160;</td>
          <td class="paramname"><em>summary_evaluators</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[dict] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[int] &#160;</td>
          <td class="paramname"><em>max_concurrency</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classlangsmith_1_1client_1_1Client.html">langsmith.Client</a>] &#160;</td>
          <td class="paramname"><em>client</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>load_nested</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>blocking</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Evaluate existing experiment runs.

Args:
    experiment (Union[str, uuid.UUID]): The identifier of the experiment to evaluate.
    data (DATA_T): The data to use for evaluation.
    evaluators (Optional[Sequence[EVALUATOR_T]]): Optional sequence of evaluators to use for individual run evaluation.
    summary_evaluators (Optional[Sequence[SUMMARY_EVALUATOR_T]]): Optional sequence of evaluators
        to apply over the entire dataset.
    metadata (Optional[dict]): Optional metadata to include in the evaluation results.
    max_concurrency (int | None): The maximum number of concurrent
        evaluations to run. If None then no limit is set. If 0 then no concurrency.
        Defaults to 0.
    client (Optional[langsmith.Client]): Optional Langsmith client to use for evaluation.
    load_nested: Whether to load all child runs for the experiment.
        Default is to only load the top-level root runs.
    blocking (bool): Whether to block until evaluation is complete.

Returns:
    ExperimentResults: The evaluation results.

Environment:
    - LANGSMITH_TEST_CACHE: If set, API calls will be cached to disk to save time and
        cost during testing. Recommended to commit the cache files to your repository
        for faster CI/CD runs.
        Requires the 'langsmith[vcr]' package to be installed.

Examples:
    &gt;&gt;&gt; from langsmith.evaluation import evaluate, evaluate_existing
    &gt;&gt;&gt; dataset_name = "Evaluate Examples"
    &gt;&gt;&gt; def predict(inputs: dict) -&gt; dict:
    ...     # This can be any function or just an API call to your app.
    ...     return {"output": "Yes"}
    &gt;&gt;&gt; # First run inference on the dataset
    ... results = evaluate(
    ...     predict,
    ...     data=dataset_name,
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...
    &gt;&gt;&gt; # Then apply evaluators to the experiment
    ... def accuracy(run: Run, example: Example):
    ...     # Row-level evaluator for accuracy.
    ...     pred = run.outputs["output"]
    ...     expected = example.outputs["answer"]
    ...     return {"score": expected.lower() == pred.lower()}
    &gt;&gt;&gt; def precision(runs: Sequence[Run], examples: Sequence[Example]):
    ...     # Experiment-level evaluator for precision.
    ...     # TP / (TP + FP)
    ...     predictions = [run.outputs["output"].lower() for run in runs]
    ...     expected = [example.outputs["answer"].lower() for example in examples]
    ...     # yes and no are the only possible answers
    ...     tp = sum([p == e for p, e in zip(predictions, expected) if p == "yes"])
    ...     fp = sum([p == "yes" and e == "no" for p, e in zip(predictions, expected)])
    ...     return {"score": tp / (tp + fp)}
    &gt;&gt;&gt; experiment_name = (
    ...     results.experiment_name
    ... )  # Can use the returned experiment name
    &gt;&gt;&gt; experiment_name = "My Experiment:64e6e91"  # Or manually specify
    &gt;&gt;&gt; results = evaluate_existing(
    ...     experiment_name,
    ...     summary_evaluators=[precision],
    ... )  # doctest: +ELLIPSIS
    View the evaluation results for experiment:...
</pre> 
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a51bf18c49d3178827c75d13d1a92106f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a51bf18c49d3178827c75d13d1a92106f">&#9670;&nbsp;</a></span>AEVALUATOR_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.AEVALUATOR_T</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;=  Union[</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;    Callable[</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;        [schemas.Run, Optional[schemas.Example]],</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;        Awaitable[Union[EvaluationResult, EvaluationResults]],</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;    ],</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;]</div>
</div><!-- fragment -->
</div>
</div>
<a id="ab58b15820ef3294c384dd205030adfe3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab58b15820ef3294c384dd205030adfe3">&#9670;&nbsp;</a></span>COMPARATIVE_EVALUATOR_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.COMPARATIVE_EVALUATOR_T</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;=  Callable[</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;    [Sequence[schemas.Run], Optional[schemas.Example]],</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;    Union[</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;        Union[ComparisonEvaluationResult, dict],</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;        Awaitable[Union[ComparisonEvaluationResult, dict]],</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;    ],</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;]</div>
</div><!-- fragment -->
<p>Public API for Comparison Experiments. </p>

</div>
</div>
<a id="a95583b1467514610041306fb757e4ae4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a95583b1467514610041306fb757e4ae4">&#9670;&nbsp;</a></span>DATA_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.DATA_T = Union[str, uuid.UUID, Iterable[<a class="el" href="classlangsmith_1_1schemas_1_1Example.html">schemas.Example</a>], <a class="el" href="classlangsmith_1_1schemas_1_1Dataset.html">schemas.Dataset</a>]</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a30b1fe35f0a53551b52534ed6a150369"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a30b1fe35f0a53551b52534ed6a150369">&#9670;&nbsp;</a></span>DataFrame</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.DataFrame = pd.DataFrame</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aad6de50c4c105251c50847d0602a8516"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad6de50c4c105251c50847d0602a8516">&#9670;&nbsp;</a></span>ET</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.ET = TypeVar(&quot;ET&quot;, bound=&quot;_ExperimentManagerMixin&quot;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a80d4847aaa84cf5c2817d8a9a997b9b7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80d4847aaa84cf5c2817d8a9a997b9b7">&#9670;&nbsp;</a></span>EVALUATOR_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.EVALUATOR_T</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;=  Union[</div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;    RunEvaluator,</div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;    Callable[</div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;        [schemas.Run, Optional[schemas.Example]],</div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;        Union[EvaluationResult, EvaluationResults],</div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;    ],</div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;    Callable[..., Union[dict, EvaluationResults, EvaluationResult]],</div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;]</div>
</div><!-- fragment -->
</div>
</div>
<a id="aab1d851700eca72667fbe5e8d530f2ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aab1d851700eca72667fbe5e8d530f2ef">&#9670;&nbsp;</a></span>EXPERIMENT_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.EXPERIMENT_T = Union[str, uuid.UUID, <a class="el" href="classlangsmith_1_1schemas_1_1TracerSession.html">schemas.TracerSession</a>]</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a223797b0a19a159f33fcdcc12a8be7ce"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a223797b0a19a159f33fcdcc12a8be7ce">&#9670;&nbsp;</a></span>IT</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.IT = TypeVar(&quot;IT&quot;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="abb69e11a442f56b1a13733f94739852b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abb69e11a442f56b1a13733f94739852b">&#9670;&nbsp;</a></span>logger</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.logger = logging.getLogger(__name__)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab4113c8825ee2565827ed474da561160"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab4113c8825ee2565827ed474da561160">&#9670;&nbsp;</a></span>TARGET_T</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langsmith.evaluation._runner.TARGET_T = Union[Callable[[dict], dict], Callable[[dict, dict], dict]]</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
