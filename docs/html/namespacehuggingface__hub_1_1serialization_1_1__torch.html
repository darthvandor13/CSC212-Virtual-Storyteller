<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: huggingface_hub.serialization._torch Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacehuggingface__hub.html">huggingface_hub</a></li><li class="navelem"><a class="el" href="namespacehuggingface__hub_1_1serialization.html">serialization</a></li><li class="navelem"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html">_torch</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">huggingface_hub.serialization._torch Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classhuggingface__hub_1_1serialization_1_1__torch_1_1__IncompatibleKeys.html">_IncompatibleKeys</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a122dbf22ae38c731f6511253b65b8657"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a122dbf22ae38c731f6511253b65b8657">save_torch_model</a> (&quot;torch.nn.Module&quot; model, Union[str, Path] save_directory, *Optional[str] filename_pattern=None, bool force_contiguous=True, Union[int, str] max_shard_size=MAX_SHARD_SIZE, Optional[Dict[str, str]] metadata=None, bool safe_serialization=True, bool is_main_process=True, Optional[List[str]] shared_tensors_to_discard=None)</td></tr>
<tr class="separator:a122dbf22ae38c731f6511253b65b8657"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0844137d116d639c3a31dca3096f4d38"><td class="memItemLeft" align="right" valign="top">None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a0844137d116d639c3a31dca3096f4d38">save_torch_state_dict</a> (Dict[str, &quot;torch.Tensor&quot;] state_dict, Union[str, Path] save_directory, *Optional[str] filename_pattern=None, bool force_contiguous=True, Union[int, str] max_shard_size=MAX_SHARD_SIZE, Optional[Dict[str, str]] metadata=None, bool safe_serialization=True, bool is_main_process=True, Optional[List[str]] shared_tensors_to_discard=None)</td></tr>
<tr class="separator:a0844137d116d639c3a31dca3096f4d38"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a8007881882a4437a5062271bd12b0c"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classhuggingface__hub_1_1serialization_1_1__base_1_1StateDictSplit.html">StateDictSplit</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a5a8007881882a4437a5062271bd12b0c">split_torch_state_dict_into_shards</a> (Dict[str, &quot;torch.Tensor&quot;] state_dict, *str filename_pattern=<a class="el" href="namespacehuggingface__hub_1_1constants.html#a60b6d336beaa2b2db0337ea8a430a701">constants.SAFETENSORS_WEIGHTS_FILE_PATTERN</a>, Union[int, str] max_shard_size=MAX_SHARD_SIZE)</td></tr>
<tr class="separator:a5a8007881882a4437a5062271bd12b0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4c358597dc719649d851d1c90f9ba1c8"><td class="memItemLeft" align="right" valign="top">NamedTuple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a4c358597dc719649d851d1c90f9ba1c8">load_torch_model</a> (&quot;torch.nn.Module&quot; model, Union[str, os.PathLike] checkpoint_path, *bool strict=False, bool safe=True, bool weights_only=False, Optional[Union[str, &quot;torch.device&quot;]] map_location=None, bool mmap=False, Optional[str] filename_pattern=None)</td></tr>
<tr class="separator:a4c358597dc719649d851d1c90f9ba1c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac4fa03d3be15ba99ab7335e0c2f9f4ea"><td class="memItemLeft" align="right" valign="top">Union[Dict[str, &quot;torch.Tensor&quot;], Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#ac4fa03d3be15ba99ab7335e0c2f9f4ea">load_state_dict_from_file</a> (Union[str, os.PathLike] checkpoint_file, Optional[Union[str, &quot;torch.device&quot;]] map_location=None, bool weights_only=False, bool mmap=False)</td></tr>
<tr class="separator:ac4fa03d3be15ba99ab7335e0c2f9f4ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a027049b98929e9f2165626f0b011fd80"><td class="memItemLeft" align="right" valign="top">Optional[Tuple[&quot;torch.device&quot;, Union[int, Tuple[Any,...]], int]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a027049b98929e9f2165626f0b011fd80">get_torch_storage_id</a> (&quot;torch.Tensor&quot; tensor)</td></tr>
<tr class="separator:a027049b98929e9f2165626f0b011fd80"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af16baf0d9918226fe06bf609cfbc08c0"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#af16baf0d9918226fe06bf609cfbc08c0">get_torch_storage_size</a> (&quot;torch.Tensor&quot; tensor)</td></tr>
<tr class="separator:af16baf0d9918226fe06bf609cfbc08c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a81458455847f1b3f31cfde3375a51ca7"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a81458455847f1b3f31cfde3375a51ca7">is_torch_tpu_available</a> (check_device=True)</td></tr>
<tr class="separator:a81458455847f1b3f31cfde3375a51ca7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a65de4bbe799710c50627953759a7f87d"><td class="memItemLeft" align="right" valign="top">Union[int, Tuple[Any,...]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a65de4bbe799710c50627953759a7f87d">storage_ptr</a> (&quot;torch.Tensor&quot; tensor)</td></tr>
<tr class="separator:a65de4bbe799710c50627953759a7f87d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a0779b637727b55f32940dd1a44794194"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacehuggingface__hub_1_1serialization_1_1__torch.html#a0779b637727b55f32940dd1a44794194">logger</a> = logging.get_logger(__file__)</td></tr>
<tr class="separator:a0779b637727b55f32940dd1a44794194"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a027049b98929e9f2165626f0b011fd80"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a027049b98929e9f2165626f0b011fd80">&#9670;&nbsp;</a></span>get_torch_storage_id()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Optional[Tuple[&quot;torch.device&quot;, Union[int, Tuple[Any, ...]], int]] huggingface_hub.serialization._torch.get_torch_storage_id </td>
          <td>(</td>
          <td class="paramtype">&quot;torch.Tensor&quot;&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Return unique identifier to a tensor storage.

Multiple different tensors can share the same underlying storage. This identifier is
guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with
non-overlapping lifetimes may have the same id.
In the case of meta tensors, we return None since we can't tell if they share the same storage.

Taken from https://github.com/huggingface/transformers/blob/1ecf5f7c982d761b4daaa96719d162c324187c64/src/transformers/pytorch_utils.py#L278.
</pre> 
</div>
</div>
<a id="af16baf0d9918226fe06bf609cfbc08c0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af16baf0d9918226fe06bf609cfbc08c0">&#9670;&nbsp;</a></span>get_torch_storage_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int huggingface_hub.serialization._torch.get_torch_storage_size </td>
          <td>(</td>
          <td class="paramtype">&quot;torch.Tensor&quot;&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Taken from https://github.com/huggingface/safetensors/blob/08db34094e9e59e2f9218f2df133b7b4aaff5a99/bindings/python/py_src/safetensors/torch.py#L31C1-L41C59
</pre> 
</div>
</div>
<a id="a81458455847f1b3f31cfde3375a51ca7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a81458455847f1b3f31cfde3375a51ca7">&#9670;&nbsp;</a></span>is_torch_tpu_available()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def huggingface_hub.serialization._torch.is_torch_tpu_available </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>check_device</em> = <code>True</code></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Checks if `torch_xla` is installed and potentially if a TPU is in the environment

Taken from https://github.com/huggingface/transformers/blob/1ecf5f7c982d761b4daaa96719d162c324187c64/src/transformers/utils/import_utils.py#L463.
</pre> 
</div>
</div>
<a id="ac4fa03d3be15ba99ab7335e0c2f9f4ea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac4fa03d3be15ba99ab7335e0c2f9f4ea">&#9670;&nbsp;</a></span>load_state_dict_from_file()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[Dict[str, &quot;torch.Tensor&quot;], Any] huggingface_hub.serialization._torch.load_state_dict_from_file </td>
          <td>(</td>
          <td class="paramtype">Union[str, os.PathLike]&#160;</td>
          <td class="paramname"><em>checkpoint_file</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[str, &quot;torch.device&quot;]] &#160;</td>
          <td class="paramname"><em>map_location</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>weights_only</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>mmap</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Loads a checkpoint file, handling both safetensors and pickle checkpoint formats.

Args:
    checkpoint_file (`str` or `os.PathLike`):
        Path to the checkpoint file to load. Can be either a safetensors or pickle (`.bin`) checkpoint.
    map_location (`str` or `torch.device`, *optional*):
        A `torch.device` object, string or a dict specifying how to remap storage locations. It
        indicates the location where all tensors should be loaded.
    weights_only (`bool`, *optional*, defaults to `False`):
        If True, only loads the model weights without optimizer states and other metadata.
        Only supported for pickle (`.bin`) checkpoints with PyTorch &gt;= 1.13. Has no effect when
        loading safetensors files.
    mmap (`bool`, *optional*, defaults to `False`):
        Whether to use memory-mapped file loading. Memory mapping can improve loading performance
        for large models in PyTorch &gt;= 2.1.0 with zipfile-based checkpoints. Has no effect when
        loading safetensors files, as the `safetensors` library uses memory mapping by default.

Returns:
    `Union[Dict[str, "torch.Tensor"], Any]`: The loaded checkpoint.
        - For safetensors files: always returns a dictionary mapping parameter names to tensors.
        - For pickle files: returns any Python object that was pickled (commonly a state dict, but could be
          an entire model, optimizer state, or any other Python object).

Raises:
    [`FileNotFoundError`](https://docs.python.org/3/library/exceptions.html#FileNotFoundError)
        If the checkpoint file does not exist.
    [`ImportError`](https://docs.python.org/3/library/exceptions.html#ImportError)
        If safetensors or torch is not installed when trying to load a .safetensors file or a PyTorch checkpoint respectively.
    [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
        If the checkpoint file format is invalid or if git-lfs files are not properly downloaded.
    [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
        If the checkpoint file path is empty or invalid.

Example:
```python
&gt;&gt;&gt; from huggingface_hub import load_state_dict_from_file

# Load a PyTorch checkpoint
&gt;&gt;&gt; state_dict = load_state_dict_from_file("path/to/model.bin", map_location="cpu")
&gt;&gt;&gt; model.load_state_dict(state_dict)

# Load a safetensors checkpoint
&gt;&gt;&gt; state_dict = load_state_dict_from_file("path/to/model.safetensors")
&gt;&gt;&gt; model.load_state_dict(state_dict)
```
</pre> 
</div>
</div>
<a id="a4c358597dc719649d851d1c90f9ba1c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4c358597dc719649d851d1c90f9ba1c8">&#9670;&nbsp;</a></span>load_torch_model()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">NamedTuple huggingface_hub.serialization._torch.load_torch_model </td>
          <td>(</td>
          <td class="paramtype">&quot;torch.nn.Module&quot;&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, os.PathLike]&#160;</td>
          <td class="paramname"><em>checkpoint_path</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*bool &#160;</td>
          <td class="paramname"><em>strict</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>safe</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>weights_only</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[str, &quot;torch.device&quot;]] &#160;</td>
          <td class="paramname"><em>map_location</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>mmap</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>filename_pattern</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Load a checkpoint into a model, handling both sharded and non-sharded checkpoints.

Args:
    model (`torch.nn.Module`):
        The model in which to load the checkpoint.
    checkpoint_path (`str` or `os.PathLike`):
        Path to either the checkpoint file or directory containing the checkpoint(s).
    strict (`bool`, *optional*, defaults to `False`):
        Whether to strictly enforce that the keys in the model state dict match the keys in the checkpoint.
    safe (`bool`, *optional*, defaults to `True`):
        If `safe` is True, the safetensors files will be loaded. If `safe` is False, the function
        will first attempt to load safetensors files if they are available, otherwise it will fall back to loading
        pickle files. `filename_pattern` parameter takes precedence over `safe` parameter.
    weights_only (`bool`, *optional*, defaults to `False`):
        If True, only loads the model weights without optimizer states and other metadata.
        Only supported in PyTorch &gt;= 1.13.
    map_location (`str` or `torch.device`, *optional*):
        A `torch.device` object, string or a dict specifying how to remap storage locations. It
        indicates the location where all tensors should be loaded.
    mmap (`bool`, *optional*, defaults to `False`):
        Whether to use memory-mapped file loading. Memory mapping can improve loading performance
        for large models in PyTorch &gt;= 2.1.0 with zipfile-based checkpoints.
    filename_pattern (`str`, *optional*):
        The pattern to look for the index file. Pattern must be a string that
        can be formatted with `filename_pattern.format(suffix=...)` and must contain the keyword `suffix`
        Defaults to `"model{suffix}.safetensors"`.
Returns:
    `NamedTuple`: A named tuple with `missing_keys` and `unexpected_keys` fields.
        - `missing_keys` is a list of str containing the missing keys, i.e. keys that are in the model but not in the checkpoint.
        - `unexpected_keys` is a list of str containing the unexpected keys, i.e. keys that are in the checkpoint but not in the model.

Raises:
    [`FileNotFoundError`](https://docs.python.org/3/library/exceptions.html#FileNotFoundError)
        If the checkpoint file or directory does not exist.
    [`ImportError`](https://docs.python.org/3/library/exceptions.html#ImportError)
        If safetensors or torch is not installed when trying to load a .safetensors file or a PyTorch checkpoint respectively.
    [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
       If the checkpoint path is invalid or if the checkpoint format cannot be determined.

Example:
```python
&gt;&gt;&gt; from huggingface_hub import load_torch_model
&gt;&gt;&gt; model = ... # A PyTorch model
&gt;&gt;&gt; load_torch_model(model, "path/to/checkpoint")
```
</pre> 
</div>
</div>
<a id="a122dbf22ae38c731f6511253b65b8657"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a122dbf22ae38c731f6511253b65b8657">&#9670;&nbsp;</a></span>save_torch_model()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def huggingface_hub.serialization._torch.save_torch_model </td>
          <td>(</td>
          <td class="paramtype">&quot;torch.nn.Module&quot;&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, Path]&#160;</td>
          <td class="paramname"><em>save_directory</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>filename_pattern</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>force_contiguous</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>max_shard_size</em> = <code>MAX_SHARD_SIZE</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, str]] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>safe_serialization</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_main_process</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[List[str]] &#160;</td>
          <td class="paramname"><em>shared_tensors_to_discard</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Saves a given torch model to disk, handling sharding and shared tensors issues.

See also [`save_torch_state_dict`] to save a state dict with more flexibility.

For more information about tensor sharing, check out [this guide](https://huggingface.co/docs/safetensors/torch_shared_tensors).

The model state dictionary is split into shards so that each shard is smaller than a given size. The shards are
saved in the `save_directory` with the given `filename_pattern`. If the model is too big to fit in a single shard,
an index file is saved in the `save_directory` to indicate where each tensor is saved. This helper uses
[`split_torch_state_dict_into_shards`] under the hood. If `safe_serialization` is `True`, the shards are saved as
safetensors (the default). Otherwise, the shards are saved as pickle.

Before saving the model, the `save_directory` is cleaned from any previous shard files.

&lt;Tip warning={true}&gt;

If one of the model's tensor is bigger than `max_shard_size`, it will end up in its own shard which will have a
size greater than `max_shard_size`.

&lt;/Tip&gt;

&lt;Tip warning={true}&gt;

If your model is a `transformers.PreTrainedModel`, you should pass `model._tied_weights_keys` as `shared_tensors_to_discard` to properly handle shared tensors saving. This ensures the correct duplicate tensors are discarded during saving.

&lt;/Tip&gt;

Args:
    model (`torch.nn.Module`):
        The model to save on disk.
    save_directory (`str` or `Path`):
        The directory in which the model will be saved.
    filename_pattern (`str`, *optional*):
        The pattern to generate the files names in which the model will be saved. Pattern must be a string that
        can be formatted with `filename_pattern.format(suffix=...)` and must contain the keyword `suffix`
        Defaults to `"model{suffix}.safetensors"` or `pytorch_model{suffix}.bin` depending on `safe_serialization`
        parameter.
    force_contiguous (`boolean`, *optional*):
        Forcing the state_dict to be saved as contiguous tensors. This has no effect on the correctness of the
        model, but it could potentially change performance if the layout of the tensor was chosen specifically for
        that reason. Defaults to `True`.
    max_shard_size (`int` or `str`, *optional*):
        The maximum size of each shard, in bytes. Defaults to 5GB.
    metadata (`Dict[str, str]`, *optional*):
        Extra information to save along with the model. Some metadata will be added for each dropped tensors.
        This information will not be enough to recover the entire shared structure but might help understanding
        things.
    safe_serialization (`bool`, *optional*):
        Whether to save as safetensors, which is the default behavior. If `False`, the shards are saved as pickle.
        Safe serialization is recommended for security reasons. Saving as pickle is deprecated and will be removed
        in a future version.
    is_main_process (`bool`, *optional*):
        Whether the process calling this is the main process or not. Useful when in distributed training like
        TPUs and need to call this function from all processes. In this case, set `is_main_process=True` only on
        the main process to avoid race conditions. Defaults to True.
    shared_tensors_to_discard (`List[str]`, *optional*):
        List of tensor names to drop when saving shared tensors. If not provided and shared tensors are
        detected, it will drop the first name alphabetically.

Example:

```py
&gt;&gt;&gt; from huggingface_hub import save_torch_model
&gt;&gt;&gt; model = ... # A PyTorch model

# Save state dict to "path/to/folder". The model will be split into shards of 5GB each and saved as safetensors.
&gt;&gt;&gt; save_torch_model(model, "path/to/folder")

# Load model back
&gt;&gt;&gt; from huggingface_hub import load_torch_model  # TODO
&gt;&gt;&gt; load_torch_model(model, "path/to/folder")
&gt;&gt;&gt;
```
</pre> 
</div>
</div>
<a id="a0844137d116d639c3a31dca3096f4d38"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0844137d116d639c3a31dca3096f4d38">&#9670;&nbsp;</a></span>save_torch_state_dict()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">None huggingface_hub.serialization._torch.save_torch_state_dict </td>
          <td>(</td>
          <td class="paramtype">Dict[str, &quot;torch.Tensor&quot;]&#160;</td>
          <td class="paramname"><em>state_dict</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, Path]&#160;</td>
          <td class="paramname"><em>save_directory</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>filename_pattern</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>force_contiguous</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>max_shard_size</em> = <code>MAX_SHARD_SIZE</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, str]] &#160;</td>
          <td class="paramname"><em>metadata</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>safe_serialization</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_main_process</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[List[str]] &#160;</td>
          <td class="paramname"><em>shared_tensors_to_discard</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Save a model state dictionary to the disk, handling sharding and shared tensors issues.

See also [`save_torch_model`] to directly save a PyTorch model.

For more information about tensor sharing, check out [this guide](https://huggingface.co/docs/safetensors/torch_shared_tensors).

The model state dictionary is split into shards so that each shard is smaller than a given size. The shards are
saved in the `save_directory` with the given `filename_pattern`. If the model is too big to fit in a single shard,
an index file is saved in the `save_directory` to indicate where each tensor is saved. This helper uses
[`split_torch_state_dict_into_shards`] under the hood. If `safe_serialization` is `True`, the shards are saved as
safetensors (the default). Otherwise, the shards are saved as pickle.

Before saving the model, the `save_directory` is cleaned from any previous shard files.

&lt;Tip warning={true}&gt;

If one of the model's tensor is bigger than `max_shard_size`, it will end up in its own shard which will have a
size greater than `max_shard_size`.

&lt;/Tip&gt;

&lt;Tip warning={true}&gt;

If your model is a `transformers.PreTrainedModel`, you should pass `model._tied_weights_keys` as `shared_tensors_to_discard` to properly handle shared tensors saving. This ensures the correct duplicate tensors are discarded during saving.

&lt;/Tip&gt;

Args:
    state_dict (`Dict[str, torch.Tensor]`):
        The state dictionary to save.
    save_directory (`str` or `Path`):
        The directory in which the model will be saved.
    filename_pattern (`str`, *optional*):
        The pattern to generate the files names in which the model will be saved. Pattern must be a string that
        can be formatted with `filename_pattern.format(suffix=...)` and must contain the keyword `suffix`
        Defaults to `"model{suffix}.safetensors"` or `pytorch_model{suffix}.bin` depending on `safe_serialization`
        parameter.
    force_contiguous (`boolean`, *optional*):
        Forcing the state_dict to be saved as contiguous tensors. This has no effect on the correctness of the
        model, but it could potentially change performance if the layout of the tensor was chosen specifically for
        that reason. Defaults to `True`.
    max_shard_size (`int` or `str`, *optional*):
        The maximum size of each shard, in bytes. Defaults to 5GB.
    metadata (`Dict[str, str]`, *optional*):
        Extra information to save along with the model. Some metadata will be added for each dropped tensors.
        This information will not be enough to recover the entire shared structure but might help understanding
        things.
    safe_serialization (`bool`, *optional*):
        Whether to save as safetensors, which is the default behavior. If `False`, the shards are saved as pickle.
        Safe serialization is recommended for security reasons. Saving as pickle is deprecated and will be removed
        in a future version.
    is_main_process (`bool`, *optional*):
        Whether the process calling this is the main process or not. Useful when in distributed training like
        TPUs and need to call this function from all processes. In this case, set `is_main_process=True` only on
        the main process to avoid race conditions. Defaults to True.
    shared_tensors_to_discard (`List[str]`, *optional*):
        List of tensor names to drop when saving shared tensors. If not provided and shared tensors are
        detected, it will drop the first name alphabetically.

Example:

```py
&gt;&gt;&gt; from huggingface_hub import save_torch_state_dict
&gt;&gt;&gt; model = ... # A PyTorch model

# Save state dict to "path/to/folder". The model will be split into shards of 5GB each and saved as safetensors.
&gt;&gt;&gt; state_dict = model_to_save.state_dict()
&gt;&gt;&gt; save_torch_state_dict(state_dict, "path/to/folder")
```
</pre> 
</div>
</div>
<a id="a5a8007881882a4437a5062271bd12b0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a8007881882a4437a5062271bd12b0c">&#9670;&nbsp;</a></span>split_torch_state_dict_into_shards()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classhuggingface__hub_1_1serialization_1_1__base_1_1StateDictSplit.html">StateDictSplit</a> huggingface_hub.serialization._torch.split_torch_state_dict_into_shards </td>
          <td>(</td>
          <td class="paramtype">Dict[str, &quot;torch.Tensor&quot;]&#160;</td>
          <td class="paramname"><em>state_dict</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*str &#160;</td>
          <td class="paramname"><em>filename_pattern</em> = <code><a class="el" href="namespacehuggingface__hub_1_1constants.html#a60b6d336beaa2b2db0337ea8a430a701">constants.SAFETENSORS_WEIGHTS_FILE_PATTERN</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[int, str] &#160;</td>
          <td class="paramname"><em>max_shard_size</em> = <code>MAX_SHARD_SIZE</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Split a model state dictionary in shards so that each shard is smaller than a given size.

The shards are determined by iterating through the `state_dict` in the order of its keys. There is no optimization
made to make each shard as close as possible to the maximum size passed. For example, if the limit is 10GB and we
have tensors of sizes [6GB, 6GB, 2GB, 6GB, 2GB, 2GB] they will get sharded as [6GB], [6+2GB], [6+2+2GB] and not
[6+2+2GB], [6+2GB], [6GB].


&lt;Tip&gt;

To save a model state dictionary to the disk, see [`save_torch_state_dict`]. This helper uses
`split_torch_state_dict_into_shards` under the hood.

&lt;/Tip&gt;

&lt;Tip warning={true}&gt;

If one of the model's tensor is bigger than `max_shard_size`, it will end up in its own shard which will have a
size greater than `max_shard_size`.

&lt;/Tip&gt;

Args:
    state_dict (`Dict[str, torch.Tensor]`):
        The state dictionary to save.
    filename_pattern (`str`, *optional*):
        The pattern to generate the files names in which the model will be saved. Pattern must be a string that
        can be formatted with `filename_pattern.format(suffix=...)` and must contain the keyword `suffix`
        Defaults to `"model{suffix}.safetensors"`.
    max_shard_size (`int` or `str`, *optional*):
        The maximum size of each shard, in bytes. Defaults to 5GB.

Returns:
    [`StateDictSplit`]: A `StateDictSplit` object containing the shards and the index to retrieve them.

Example:
```py
&gt;&gt;&gt; import json
&gt;&gt;&gt; import os
&gt;&gt;&gt; from safetensors.torch import save_file as safe_save_file
&gt;&gt;&gt; from huggingface_hub import split_torch_state_dict_into_shards

&gt;&gt;&gt; def save_state_dict(state_dict: Dict[str, torch.Tensor], save_directory: str):
...     state_dict_split = split_torch_state_dict_into_shards(state_dict)
...     for filename, tensors in state_dict_split.filename_to_tensors.items():
...         shard = {tensor: state_dict[tensor] for tensor in tensors}
...         safe_save_file(
...             shard,
...             os.path.join(save_directory, filename),
...             metadata={"format": "pt"},
...         )
...     if state_dict_split.is_sharded:
...         index = {
...             "metadata": state_dict_split.metadata,
...             "weight_map": state_dict_split.tensor_to_filename,
...         }
...         with open(os.path.join(save_directory, "model.safetensors.index.json"), "w") as f:
...             f.write(json.dumps(index, indent=2))
```
</pre> 
</div>
</div>
<a id="a65de4bbe799710c50627953759a7f87d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a65de4bbe799710c50627953759a7f87d">&#9670;&nbsp;</a></span>storage_ptr()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[int, Tuple[Any, ...]] huggingface_hub.serialization._torch.storage_ptr </td>
          <td>(</td>
          <td class="paramtype">&quot;torch.Tensor&quot;&#160;</td>
          <td class="paramname"><em>tensor</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Taken from https://github.com/huggingface/safetensors/blob/079781fd0dc455ba0fe851e2b4507c33d0c0d407/bindings/python/py_src/safetensors/torch.py#L11.
</pre> 
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a0779b637727b55f32940dd1a44794194"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0779b637727b55f32940dd1a44794194">&#9670;&nbsp;</a></span>logger</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">huggingface_hub.serialization._torch.logger = logging.get_logger(__file__)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
