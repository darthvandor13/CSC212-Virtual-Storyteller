<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: langchain_core.language_models.llms Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacelangchain__core.html">langchain_core</a></li><li class="navelem"><a class="el" href="namespacelangchain__core_1_1language__models.html">language_models</a></li><li class="navelem"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html">llms</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">langchain_core.language_models.llms Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangchain__core_1_1language__models_1_1llms_1_1BaseLLM.html">BaseLLM</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangchain__core_1_1language__models_1_1llms_1_1LLM.html">LLM</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:af2eecd849dc34231461bedcf81d1bfb3"><td class="memItemLeft" align="right" valign="top">Callable[[Any], Any]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#af2eecd849dc34231461bedcf81d1bfb3">create_base_retry_decorator</a> (list[type[BaseException]] error_types, int max_retries=1, Optional[Union[<a class="el" href="classlangchain__core_1_1callbacks_1_1manager_1_1AsyncCallbackManagerForLLMRun.html">AsyncCallbackManagerForLLMRun</a>, <a class="el" href="classlangchain__core_1_1callbacks_1_1manager_1_1CallbackManagerForLLMRun.html">CallbackManagerForLLMRun</a>]] run_manager=None)</td></tr>
<tr class="separator:af2eecd849dc34231461bedcf81d1bfb3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0d3cbe3f5abaf0dd7ba440b0960682d3"><td class="memItemLeft" align="right" valign="top">tuple[dict[int, list], str, list[int], list[str]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#a0d3cbe3f5abaf0dd7ba440b0960682d3">get_prompts</a> (dict[str, Any] params, list[str] prompts, Optional[Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]] cache=None)</td></tr>
<tr class="separator:a0d3cbe3f5abaf0dd7ba440b0960682d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a77758ad1d9c775a3f8a527fcd71c8b84"><td class="memItemLeft" align="right" valign="top">tuple[dict[int, list], str, list[int], list[str]]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#a77758ad1d9c775a3f8a527fcd71c8b84">aget_prompts</a> (dict[str, Any] params, list[str] prompts, Optional[Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]] cache=None)</td></tr>
<tr class="separator:a77758ad1d9c775a3f8a527fcd71c8b84"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ce6193680c56793a69541eb00516b08"><td class="memItemLeft" align="right" valign="top">Optional[dict]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#a7ce6193680c56793a69541eb00516b08">update_cache</a> (Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None] cache, dict[int, list] existing_prompts, str llm_string, list[int] missing_prompt_idxs, LLMResult new_results, list[str] prompts)</td></tr>
<tr class="separator:a7ce6193680c56793a69541eb00516b08"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac99687a14317fe7e4fc64cbcaf677c9b"><td class="memItemLeft" align="right" valign="top">Optional[dict]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#ac99687a14317fe7e4fc64cbcaf677c9b">aupdate_cache</a> (Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None] cache, dict[int, list] existing_prompts, str llm_string, list[int] missing_prompt_idxs, LLMResult new_results, list[str] prompts)</td></tr>
<tr class="separator:ac99687a14317fe7e4fc64cbcaf677c9b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a1509e80cefba278e0976cd870d447d5f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain__core_1_1language__models_1_1llms.html#a1509e80cefba278e0976cd870d447d5f">logger</a> = logging.getLogger(__name__)</td></tr>
<tr class="separator:a1509e80cefba278e0976cd870d447d5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Base interface for large language models to expose.</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a77758ad1d9c775a3f8a527fcd71c8b84"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a77758ad1d9c775a3f8a527fcd71c8b84">&#9670;&nbsp;</a></span>aget_prompts()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[dict[int, list], str, list[int], list[str]] langchain_core.language_models.llms.aget_prompts </td>
          <td>(</td>
          <td class="paramtype">dict[str, Any]&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str]&#160;</td>
          <td class="paramname"><em>prompts</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]] &#160;</td>
          <td class="paramname"><em>cache</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get prompts that are already cached. Async version.

Args:
    params: Dictionary of parameters.
    prompts: List of prompts.
    cache: Cache object. Default is None.

Returns:
    A tuple of existing prompts, llm_string, missing prompt indexes,
        and missing prompts.

Raises:
    ValueError: If the cache is not set and cache is True.
</pre> 
</div>
</div>
<a id="ac99687a14317fe7e4fc64cbcaf677c9b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac99687a14317fe7e4fc64cbcaf677c9b">&#9670;&nbsp;</a></span>aupdate_cache()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Optional[dict] langchain_core.language_models.llms.aupdate_cache </td>
          <td>(</td>
          <td class="paramtype">Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]&#160;</td>
          <td class="paramname"><em>cache</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">dict[int, list]&#160;</td>
          <td class="paramname"><em>existing_prompts</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>llm_string</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[int]&#160;</td>
          <td class="paramname"><em>missing_prompt_idxs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LLMResult&#160;</td>
          <td class="paramname"><em>new_results</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str]&#160;</td>
          <td class="paramname"><em>prompts</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Update the cache and get the LLM output. Async version.

Args:
    cache: Cache object.
    existing_prompts: Dictionary of existing prompts.
    llm_string: LLM string.
    missing_prompt_idxs: List of missing prompt indexes.
    new_results: LLMResult object.
    prompts: List of prompts.

Returns:
    LLM output.

Raises:
    ValueError: If the cache is not set and cache is True.
</pre> 
</div>
</div>
<a id="af2eecd849dc34231461bedcf81d1bfb3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af2eecd849dc34231461bedcf81d1bfb3">&#9670;&nbsp;</a></span>create_base_retry_decorator()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Callable[[Any], Any] langchain_core.language_models.llms.create_base_retry_decorator </td>
          <td>(</td>
          <td class="paramtype">list[type[BaseException]]&#160;</td>
          <td class="paramname"><em>error_types</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>max_retries</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[
        Union[<a class="el" href="classlangchain__core_1_1callbacks_1_1manager_1_1AsyncCallbackManagerForLLMRun.html">AsyncCallbackManagerForLLMRun</a>, <a class="el" href="classlangchain__core_1_1callbacks_1_1manager_1_1CallbackManagerForLLMRun.html">CallbackManagerForLLMRun</a>]
    ] &#160;</td>
          <td class="paramname"><em>run_manager</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create a retry decorator for a given LLM and provided
 a list of error types.

Args:
    error_types: List of error types to retry on.
    max_retries: Number of retries. Default is 1.
    run_manager: Callback manager for the run. Default is None.

Returns:
    A retry decorator.

Raises:
    ValueError: If the cache is not set and cache is True.
</pre> 
</div>
</div>
<a id="a0d3cbe3f5abaf0dd7ba440b0960682d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0d3cbe3f5abaf0dd7ba440b0960682d3">&#9670;&nbsp;</a></span>get_prompts()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[dict[int, list], str, list[int], list[str]] langchain_core.language_models.llms.get_prompts </td>
          <td>(</td>
          <td class="paramtype">dict[str, Any]&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str]&#160;</td>
          <td class="paramname"><em>prompts</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]] &#160;</td>
          <td class="paramname"><em>cache</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get prompts that are already cached.

Args:
    params: Dictionary of parameters.
    prompts: List of prompts.
    cache: Cache object. Default is None.

Returns:
    A tuple of existing prompts, llm_string, missing prompt indexes,
        and missing prompts.

Raises:
    ValueError: If the cache is not set and cache is True.
</pre> 
</div>
</div>
<a id="a7ce6193680c56793a69541eb00516b08"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7ce6193680c56793a69541eb00516b08">&#9670;&nbsp;</a></span>update_cache()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Optional[dict] langchain_core.language_models.llms.update_cache </td>
          <td>(</td>
          <td class="paramtype">Union[<a class="el" href="classlangchain__core_1_1caches_1_1BaseCache.html">BaseCache</a>, bool, None]&#160;</td>
          <td class="paramname"><em>cache</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">dict[int, list]&#160;</td>
          <td class="paramname"><em>existing_prompts</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>llm_string</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[int]&#160;</td>
          <td class="paramname"><em>missing_prompt_idxs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">LLMResult&#160;</td>
          <td class="paramname"><em>new_results</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">list[str]&#160;</td>
          <td class="paramname"><em>prompts</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Update the cache and get the LLM output.

Args:
    cache: Cache object.
    existing_prompts: Dictionary of existing prompts.
    llm_string: LLM string.
    missing_prompt_idxs: List of missing prompt indexes.
    new_results: LLMResult object.
    prompts: List of prompts.

Returns:
    LLM output.

Raises:
    ValueError: If the cache is not set and cache is True.
</pre> 
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a1509e80cefba278e0976cd870d447d5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1509e80cefba278e0976cd870d447d5f">&#9670;&nbsp;</a></span>logger</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">langchain_core.language_models.llms.logger = logging.getLogger(__name__)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
