<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: langchain.chat_models.base Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacelangchain.html">langchain</a></li><li class="navelem"><a class="el" href="namespacelangchain_1_1chat__models.html">chat_models</a></li><li class="navelem"><a class="el" href="namespacelangchain_1_1chat__models_1_1base.html">base</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">langchain.chat_models.base Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a6ce8377bfd8176d84a5a2bfbd2cc75a5"><td class="memItemLeft" align="right" valign="top">BaseChatModel&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain_1_1chat__models_1_1base.html#a6ce8377bfd8176d84a5a2bfbd2cc75a5">init_chat_model</a> (str model, *Optional[str] model_provider=None, Literal[None] configurable_fields=None, Optional[str] config_prefix=None, **Any kwargs)</td></tr>
<tr class="separator:a6ce8377bfd8176d84a5a2bfbd2cc75a5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae7d1cb9180af082db885de7053942c93"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain_1_1chat__models_1_1base.html#ae7d1cb9180af082db885de7053942c93">init_chat_model</a> (Literal[None] model=None, *Optional[str] model_provider=None, Literal[None] configurable_fields=None, Optional[str] config_prefix=None, **Any kwargs)</td></tr>
<tr class="separator:ae7d1cb9180af082db885de7053942c93"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e9a8c6bd426860fa606096afe15bf00"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain_1_1chat__models_1_1base.html#a2e9a8c6bd426860fa606096afe15bf00">init_chat_model</a> (Optional[str] model=None, *Optional[str] model_provider=None, Union[Literal[&quot;any&quot;], List[str], Tuple[str,...]] configurable_fields=..., Optional[str] config_prefix=None, **Any kwargs)</td></tr>
<tr class="separator:a2e9a8c6bd426860fa606096afe15bf00"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c34d0caec8fcff81d27f69141b6a58d"><td class="memItemLeft" align="right" valign="top">Union[BaseChatModel, <a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a>]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacelangchain_1_1chat__models_1_1base.html#a5c34d0caec8fcff81d27f69141b6a58d">init_chat_model</a> (Optional[str] model=None, *Optional[str] model_provider=None, Optional[Union[Literal[&quot;any&quot;], List[str], Tuple[str,...]]] configurable_fields=None, Optional[str] config_prefix=None, **Any kwargs)</td></tr>
<tr class="separator:a5c34d0caec8fcff81d27f69141b6a58d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="ae7d1cb9180af082db885de7053942c93"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae7d1cb9180af082db885de7053942c93">&#9670;&nbsp;</a></span>init_chat_model() <span class="overload">[1/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a> langchain.chat_models.base.init_chat_model </td>
          <td>(</td>
          <td class="paramtype">Literal[None] &#160;</td>
          <td class="paramname"><em>model</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>model_provider</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Literal[None] &#160;</td>
          <td class="paramname"><em>configurable_fields</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>config_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a5c34d0caec8fcff81d27f69141b6a58d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5c34d0caec8fcff81d27f69141b6a58d">&#9670;&nbsp;</a></span>init_chat_model() <span class="overload">[2/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[BaseChatModel, <a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a>] langchain.chat_models.base.init_chat_model </td>
          <td>(</td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>model</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>model_provider</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[
        Union[Literal[&quot;any&quot;], List[str], Tuple[str, ...]]
    ] &#160;</td>
          <td class="paramname"><em>configurable_fields</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>config_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Initialize a ChatModel from the model name and provider.

**Note:** Must have the integration package corresponding to the model provider
installed.

Args:
    model: The name of the model, e.g. "o3-mini", "claude-3-5-sonnet-latest". You can
        also specify model and model provider in a single argument using
        '{model_provider}:{model}' format, e.g. "openai:o1".
    model_provider: The model provider if not specified as part of model arg (see
        above). Supported model_provider values and the corresponding integration
        package are:

        - 'openai'              -&gt; langchain-openai
        - 'anthropic'           -&gt; langchain-anthropic
        - 'azure_openai'        -&gt; langchain-openai
        - 'azure_ai             -&gt; langchain-ai
        - 'google_vertexai'     -&gt; langchain-google-vertexai
        - 'google_genai'        -&gt; langchain-google-genai
        - 'bedrock'             -&gt; langchain-aws
        - 'bedrock_converse'    -&gt; langchain-aws
        - 'cohere'              -&gt; langchain-cohere
        - 'fireworks'           -&gt; langchain-fireworks
        - 'together'            -&gt; langchain-together
        - 'mistralai'           -&gt; langchain-mistralai
        - 'huggingface'         -&gt; langchain-huggingface
        - 'groq'                -&gt; langchain-groq
        - 'ollama'              -&gt; langchain-ollama
        - 'google_anthropic_vertex'    -&gt; langchain-google-vertexai
        - 'deepseek'            -&gt; langchain-deepseek
        - 'ibm'                 -&gt; langchain-ibm
        - 'nvidia'              -&gt; langchain-nvidia-ai-endpoints
        - 'xai'                 -&gt; langchain-xai

        Will attempt to infer model_provider from model if not specified. The
        following providers will be inferred based on these model prefixes:

        - 'gpt-3...' | 'gpt-4...' | 'o1...' -&gt; 'openai'
        - 'claude...'                       -&gt; 'anthropic'
        - 'amazon....'                      -&gt; 'bedrock'
        - 'gemini...'                       -&gt; 'google_vertexai'
        - 'command...'                      -&gt; 'cohere'
        - 'accounts/fireworks...'           -&gt; 'fireworks'
        - 'mistral...'                      -&gt; 'mistralai'
        - 'deepseek...'                     -&gt; 'deepseek'
        - 'grok...'                         -&gt; 'xai'
    configurable_fields: Which model parameters are
        configurable:

        - None: No configurable fields.
        - "any": All fields are configurable. *See Security Note below.*
        - Union[List[str], Tuple[str, ...]]: Specified fields are configurable.

        Fields are assumed to have config_prefix stripped if there is a
        config_prefix. If model is specified, then defaults to None. If model is
        not specified, then defaults to ``("model", "model_provider")``.

        ***Security Note***: Setting ``configurable_fields="any"`` means fields like
        api_key, base_url, etc. can be altered at runtime, potentially redirecting
        model requests to a different service/user. Make sure that if you're
        accepting untrusted configurations that you enumerate the
        ``configurable_fields=(...)`` explicitly.

    config_prefix: If config_prefix is a non-empty string then model will be
        configurable at runtime via the
        ``config["configurable"]["{config_prefix}_{param}"]`` keys. If
        config_prefix is an empty string then model will be configurable via
        ``config["configurable"]["{param}"]``.
    temperature: Model temperature.
    max_tokens: Max output tokens.
    timeout: The maximum time (in seconds) to wait for a response from the model
        before canceling the request.
    max_retries: The maximum number of attempts the system will make to resend a
        request if it fails due to issues like network timeouts or rate limits.
    base_url: The URL of the API endpoint where requests are sent.
    rate_limiter: A ``BaseRateLimiter`` to space out requests to avoid exceeding
        rate limits.
    kwargs: Additional model-specific keyword args to pass to
        ``&lt;&lt;selected ChatModel&gt;&gt;.__init__(model=model_name, **kwargs)``.

Returns:
    A BaseChatModel corresponding to the model_name and model_provider specified if
    configurability is inferred to be False. If configurable, a chat model emulator
    that initializes the underlying model at runtime once a config is passed in.

Raises:
    ValueError: If model_provider cannot be inferred or isn't supported.
    ImportError: If the model provider integration package is not installed.

.. dropdown:: Init non-configurable model
    :open:

    .. code-block:: python

        # pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai
        from langchain.chat_models import init_chat_model

        o3_mini = init_chat_model("openai:o3-mini", temperature=0)
        claude_sonnet = init_chat_model("anthropic:claude-3-5-sonnet-latest", temperature=0)
        gemini_2_flash = init_chat_model("google_vertexai:gemini-2.0-flash", temperature=0)

        o3_mini.invoke("what's your name")
        claude_sonnet.invoke("what's your name")
        gemini_2_flash.invoke("what's your name")


.. dropdown:: Partially configurable model with no default

    .. code-block:: python

        # pip install langchain langchain-openai langchain-anthropic
        from langchain.chat_models import init_chat_model

        # We don't need to specify configurable=True if a model isn't specified.
        configurable_model = init_chat_model(temperature=0)

        configurable_model.invoke(
            "what's your name",
            config={"configurable": {"model": "gpt-4o"}}
        )
        # GPT-4o response

        configurable_model.invoke(
            "what's your name",
            config={"configurable": {"model": "claude-3-5-sonnet-latest"}}
        )
        # claude-3.5 sonnet response

.. dropdown:: Fully configurable model with a default

    .. code-block:: python

        # pip install langchain langchain-openai langchain-anthropic
        from langchain.chat_models import init_chat_model

        configurable_model_with_default = init_chat_model(
            "openai:gpt-4o",
            configurable_fields="any",  # this allows us to configure other params like temperature, max_tokens, etc at runtime.
            config_prefix="foo",
            temperature=0
        )

        configurable_model_with_default.invoke("what's your name")
        # GPT-4o response with temperature 0

        configurable_model_with_default.invoke(
            "what's your name",
            config={
                "configurable": {
                    "foo_model": "anthropic:claude-3-5-sonnet-20240620",
                    "foo_temperature": 0.6
                }
            }
        )
        # Claude-3.5 sonnet response with temperature 0.6

.. dropdown:: Bind tools to a configurable model

    You can call any ChatModel declarative methods on a configurable model in the
    same way that you would with a normal model.

    .. code-block:: python

        # pip install langchain langchain-openai langchain-anthropic
        from langchain.chat_models import init_chat_model
        from pydantic import BaseModel, Field

        class GetWeather(BaseModel):
            '''Get the current weather in a given location'''

            location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

        class GetPopulation(BaseModel):
            '''Get the current population in a given location'''

            location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

        configurable_model = init_chat_model(
            "gpt-4o",
            configurable_fields=("model", "model_provider"),
            temperature=0
        )

        configurable_model_with_tools = configurable_model.bind_tools([GetWeather, GetPopulation])
        configurable_model_with_tools.invoke(
            "Which city is hotter today and which is bigger: LA or NY?"
        )
        # GPT-4o response with tool calls

        configurable_model_with_tools.invoke(
            "Which city is hotter today and which is bigger: LA or NY?",
            config={"configurable": {"model": "claude-3-5-sonnet-20240620"}}
        )
        # Claude-3.5 sonnet response with tools

.. versionadded:: 0.2.7

.. versionchanged:: 0.2.8

    Support for ``configurable_fields`` and ``config_prefix`` added.

.. versionchanged:: 0.2.12

    Support for Ollama via langchain-ollama package added
    (langchain_ollama.ChatOllama). Previously,
    the now-deprecated langchain-community version of Ollama was imported
    (langchain_community.chat_models.ChatOllama).

    Support for AWS Bedrock models via the Converse API added
    (model_provider="bedrock_converse").

.. versionchanged:: 0.3.5

    Out of beta.

.. versionchanged:: 0.3.19

    Support for Deepseek, IBM, Nvidia, and xAI models added.</pre> 
</div>
</div>
<a id="a2e9a8c6bd426860fa606096afe15bf00"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e9a8c6bd426860fa606096afe15bf00">&#9670;&nbsp;</a></span>init_chat_model() <span class="overload">[3/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classlangchain_1_1chat__models_1_1base_1_1__ConfigurableModel.html">_ConfigurableModel</a> langchain.chat_models.base.init_chat_model </td>
          <td>(</td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>model</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>model_provider</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[Literal[&quot;any&quot;], List[str], Tuple[str, ...]] &#160;</td>
          <td class="paramname"><em>configurable_fields</em> = <code>...</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>config_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6ce8377bfd8176d84a5a2bfbd2cc75a5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6ce8377bfd8176d84a5a2bfbd2cc75a5">&#9670;&nbsp;</a></span>init_chat_model() <span class="overload">[4/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> BaseChatModel langchain.chat_models.base.init_chat_model </td>
          <td>(</td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[str] &#160;</td>
          <td class="paramname"><em>model_provider</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Literal[None] &#160;</td>
          <td class="paramname"><em>configurable_fields</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] &#160;</td>
          <td class="paramname"><em>config_prefix</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**Any&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
