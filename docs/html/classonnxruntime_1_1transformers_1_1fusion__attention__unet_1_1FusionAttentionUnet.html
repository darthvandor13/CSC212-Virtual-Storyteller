<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceonnxruntime.html">onnxruntime</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1transformers.html">transformers</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1transformers_1_1fusion__attention__unet.html">fusion_attention_unet</a></li><li class="navelem"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html">FusionAttentionUnet</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet__inherit__graph.png" border="0" usemap="#aonnxruntime_8transformers_8fusion__attention__unet_8FusionAttentionUnet_inherit__map" alt="Inheritance graph"/></div>
<!-- MAP 0 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet__coll__graph.png" border="0" usemap="#aonnxruntime_8transformers_8fusion__attention__unet_8FusionAttentionUnet_coll__map" alt="Collaboration graph"/></div>
<!-- MAP 1 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a77d1f8f2c83518f3d3fe387359acd292"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a77d1f8f2c83518f3d3fe387359acd292">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aab0d4c827ae28048492bdcdc929640f9">is_cross_attention</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a2ee4e33bf9c748ed41a08e131655d96b">enable_packed_qkv</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a66b19f5e0cb7d1f57151155ada4429b2">enable_packed_kv</a>)</td></tr>
<tr class="separator:a77d1f8f2c83518f3d3fe387359acd292"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a132b8c033f0cb103c7c4d088de7e280f"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a132b8c033f0cb103c7c4d088de7e280f">get_num_heads</a> (self, NodeProto reshape_q, bool is_torch2=False)</td></tr>
<tr class="separator:a132b8c033f0cb103c7c4d088de7e280f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae515337d69de76a1669fca0730e54ac3"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ae515337d69de76a1669fca0730e54ac3">get_hidden_size</a> (self, layernorm_node)</td></tr>
<tr class="separator:ae515337d69de76a1669fca0730e54ac3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a299dc09a193b40e1ff71c3f4ed6487f1"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a299dc09a193b40e1ff71c3f4ed6487f1">get_num_heads_and_hidden_size</a> (self, NodeProto reshape_q, NodeProto layernorm_node, bool is_torch2=False)</td></tr>
<tr class="separator:a299dc09a193b40e1ff71c3f4ed6487f1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac881ea0b9dfac33723a6b37a622210e6"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac881ea0b9dfac33723a6b37a622210e6">create_attention_node</a> (self, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, str input, str output)</td></tr>
<tr class="separator:ac881ea0b9dfac33723a6b37a622210e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aefcc2c4a08f611dda28ebcd2e21e510b"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aefcc2c4a08f611dda28ebcd2e21e510b">create_attention_node_lora</a> (self, NodeProto q_matmul_add, NodeProto k_matmul_add, NodeProto v_matmul_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, str input, str output)</td></tr>
<tr class="separator:aefcc2c4a08f611dda28ebcd2e21e510b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abc747f274c53e67f0145c59f76b25807"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#abc747f274c53e67f0145c59f76b25807">fuse</a> (self, normalize_node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:abc747f274c53e67f0145c59f76b25807"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac820cf0428704e44a58fa4ab81459c4d"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac820cf0428704e44a58fa4ab81459c4d">match_qkv_torch1</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:ac820cf0428704e44a58fa4ab81459c4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac1f1fbe676691bbcc37e569ad03304f0"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac1f1fbe676691bbcc37e569ad03304f0">match_qkv_torch2</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:ac1f1fbe676691bbcc37e569ad03304f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73488e3857bdd5dee28e3ffaac8301ad"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a73488e3857bdd5dee28e3ffaac8301ad">match_qkv_torch1_lora</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a73488e3857bdd5dee28e3ffaac8301ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93fee7a83043882139d09357f3484a45"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a93fee7a83043882139d09357f3484a45">match_qkv_torch2_lora</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a93fee7a83043882139d09357f3484a45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4e9008ec2a79f9126e12529e39fc7217"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a4e9008ec2a79f9126e12529e39fc7217">match_lora_path</a> (self, NodeProto add_node)</td></tr>
<tr class="separator:a4e9008ec2a79f9126e12529e39fc7217"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a9953c208b747850da78e9998ca47c8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a1a9953c208b747850da78e9998ca47c8">fuse_a1111_fp16</a> (self, normalize_node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:a1a9953c208b747850da78e9998ca47c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c192e064c7bff6cd64bcc66ed14c1dc"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a3c192e064c7bff6cd64bcc66ed14c1dc">match_qkv_a1111</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a3c192e064c7bff6cd64bcc66ed14c1dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a77d1f8f2c83518f3d3fe387359acd292"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a77d1f8f2c83518f3d3fe387359acd292">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aab0d4c827ae28048492bdcdc929640f9">is_cross_attention</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a2ee4e33bf9c748ed41a08e131655d96b">enable_packed_qkv</a>, bool <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a66b19f5e0cb7d1f57151155ada4429b2">enable_packed_kv</a>)</td></tr>
<tr class="separator:a77d1f8f2c83518f3d3fe387359acd292"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a132b8c033f0cb103c7c4d088de7e280f"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a132b8c033f0cb103c7c4d088de7e280f">get_num_heads</a> (self, NodeProto reshape_q, bool is_torch2=False)</td></tr>
<tr class="separator:a132b8c033f0cb103c7c4d088de7e280f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae515337d69de76a1669fca0730e54ac3"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ae515337d69de76a1669fca0730e54ac3">get_hidden_size</a> (self, layernorm_node)</td></tr>
<tr class="separator:ae515337d69de76a1669fca0730e54ac3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a299dc09a193b40e1ff71c3f4ed6487f1"><td class="memItemLeft" align="right" valign="top">tuple[int, int]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a299dc09a193b40e1ff71c3f4ed6487f1">get_num_heads_and_hidden_size</a> (self, NodeProto reshape_q, NodeProto layernorm_node, bool is_torch2=False)</td></tr>
<tr class="separator:a299dc09a193b40e1ff71c3f4ed6487f1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac881ea0b9dfac33723a6b37a622210e6"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac881ea0b9dfac33723a6b37a622210e6">create_attention_node</a> (self, NodeProto q_matmul, NodeProto k_matmul, NodeProto v_matmul, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, str input, str output)</td></tr>
<tr class="separator:ac881ea0b9dfac33723a6b37a622210e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aefcc2c4a08f611dda28ebcd2e21e510b"><td class="memItemLeft" align="right" valign="top">NodeProto|None&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aefcc2c4a08f611dda28ebcd2e21e510b">create_attention_node_lora</a> (self, NodeProto q_matmul_add, NodeProto k_matmul_add, NodeProto v_matmul_add, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a>, int <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a>, str input, str output)</td></tr>
<tr class="separator:aefcc2c4a08f611dda28ebcd2e21e510b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abc747f274c53e67f0145c59f76b25807"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#abc747f274c53e67f0145c59f76b25807">fuse</a> (self, normalize_node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:abc747f274c53e67f0145c59f76b25807"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac820cf0428704e44a58fa4ab81459c4d"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac820cf0428704e44a58fa4ab81459c4d">match_qkv_torch1</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:ac820cf0428704e44a58fa4ab81459c4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac1f1fbe676691bbcc37e569ad03304f0"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ac1f1fbe676691bbcc37e569ad03304f0">match_qkv_torch2</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:ac1f1fbe676691bbcc37e569ad03304f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73488e3857bdd5dee28e3ffaac8301ad"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a73488e3857bdd5dee28e3ffaac8301ad">match_qkv_torch1_lora</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a73488e3857bdd5dee28e3ffaac8301ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93fee7a83043882139d09357f3484a45"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a93fee7a83043882139d09357f3484a45">match_qkv_torch2_lora</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a93fee7a83043882139d09357f3484a45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4e9008ec2a79f9126e12529e39fc7217"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a4e9008ec2a79f9126e12529e39fc7217">match_lora_path</a> (self, NodeProto add_node)</td></tr>
<tr class="separator:a4e9008ec2a79f9126e12529e39fc7217"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a9953c208b747850da78e9998ca47c8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a1a9953c208b747850da78e9998ca47c8">fuse_a1111_fp16</a> (self, normalize_node, input_name_to_nodes, output_name_to_node)</td></tr>
<tr class="separator:a1a9953c208b747850da78e9998ca47c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c192e064c7bff6cd64bcc66ed14c1dc"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a3c192e064c7bff6cd64bcc66ed14c1dc">match_qkv_a1111</a> (self, root_input, skip_add)</td></tr>
<tr class="separator:a3c192e064c7bff6cd64bcc66ed14c1dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html">onnxruntime.transformers.fusion_base.Fusion</a></td></tr>
<tr class="memitem:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a7d0d94dbf15fc205f4735e3d1e8828c5">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, str fused_op_type, str|list[str] search_op_types, str description=&quot;&quot;)</td></tr>
<tr class="separator:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab4ffc1eac3afcdfc42e471c9f2d41330">increase_counter</a> (self, str fused_op_name)</td></tr>
<tr class="separator:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab1bf9b45d1d3c486821b8f56fc9c3626">fuse</a> (self, NodeProto node, dict[str, list[NodeProto]] input_name_to_nodes, dict[str, NodeProto] output_name_to_node)</td></tr>
<tr class="separator:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a67b4a3359ea81d9c96c4fff4edec9fa8">apply</a> (self)</td></tr>
<tr class="separator:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a4f370334fa35b77d7667baf24ac346a1">add_initializer</a> (self, str name, int data_type, Sequence[int] dims, Any vals, bool raw=True)</td></tr>
<tr class="separator:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#aa4f74752db0beb76467a18f527234065">remove_initializer</a> (self, TensorProto tensor)</td></tr>
<tr class="separator:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a89db5ab3fec70db716d6ab2a8a30a650">add_nodes_to_remove</a> (self, list[NodeProto] nodes)</td></tr>
<tr class="separator:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a36a3534fd354c644d2b540b5e1e52e0e">add_nodes_to_remove_with_nodes_to_keep</a> (self, list[NodeProto] nodes, list[NodeProto] nodes_to_keep)</td></tr>
<tr class="separator:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a7d0d94dbf15fc205f4735e3d1e8828c5">__init__</a> (self, <a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a> model, str fused_op_type, str|list[str] search_op_types, str description=&quot;&quot;)</td></tr>
<tr class="separator:a7d0d94dbf15fc205f4735e3d1e8828c5 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab4ffc1eac3afcdfc42e471c9f2d41330">increase_counter</a> (self, str fused_op_name)</td></tr>
<tr class="separator:ab4ffc1eac3afcdfc42e471c9f2d41330 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#ab1bf9b45d1d3c486821b8f56fc9c3626">fuse</a> (self, NodeProto node, dict[str, list[NodeProto]] input_name_to_nodes, dict[str, NodeProto] output_name_to_node)</td></tr>
<tr class="separator:ab1bf9b45d1d3c486821b8f56fc9c3626 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a67b4a3359ea81d9c96c4fff4edec9fa8">apply</a> (self)</td></tr>
<tr class="separator:a67b4a3359ea81d9c96c4fff4edec9fa8 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a4f370334fa35b77d7667baf24ac346a1">add_initializer</a> (self, str name, int data_type, Sequence[int] dims, Any vals, bool raw=True)</td></tr>
<tr class="separator:a4f370334fa35b77d7667baf24ac346a1 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#aa4f74752db0beb76467a18f527234065">remove_initializer</a> (self, TensorProto tensor)</td></tr>
<tr class="separator:aa4f74752db0beb76467a18f527234065 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a89db5ab3fec70db716d6ab2a8a30a650">add_nodes_to_remove</a> (self, list[NodeProto] nodes)</td></tr>
<tr class="separator:a89db5ab3fec70db716d6ab2a8a30a650 inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a36a3534fd354c644d2b540b5e1e52e0e">add_nodes_to_remove_with_nodes_to_keep</a> (self, list[NodeProto] nodes, list[NodeProto] nodes_to_keep)</td></tr>
<tr class="separator:a36a3534fd354c644d2b540b5e1e52e0e inherit pub_methods_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:ad1d81dce13ebdaf907ffaa2456eb2713"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#ad1d81dce13ebdaf907ffaa2456eb2713">hidden_size</a></td></tr>
<tr class="separator:ad1d81dce13ebdaf907ffaa2456eb2713"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a21f415524eaf5ddbf63eb7db87d76429"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a21f415524eaf5ddbf63eb7db87d76429">num_heads</a></td></tr>
<tr class="separator:a21f415524eaf5ddbf63eb7db87d76429"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab0d4c827ae28048492bdcdc929640f9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aab0d4c827ae28048492bdcdc929640f9">is_cross_attention</a></td></tr>
<tr class="separator:aab0d4c827ae28048492bdcdc929640f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ee4e33bf9c748ed41a08e131655d96b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a2ee4e33bf9c748ed41a08e131655d96b">enable_packed_qkv</a></td></tr>
<tr class="separator:a2ee4e33bf9c748ed41a08e131655d96b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66b19f5e0cb7d1f57151155ada4429b2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a66b19f5e0cb7d1f57151155ada4429b2">enable_packed_kv</a></td></tr>
<tr class="separator:a66b19f5e0cb7d1f57151155ada4429b2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af30ec6385cc7333d918f36d85d169d29"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#af30ec6385cc7333d918f36d85d169d29">num_heads_warning</a></td></tr>
<tr class="separator:af30ec6385cc7333d918f36d85d169d29"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6251ecfed677a65d26d97f855f09dc7b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#a6251ecfed677a65d26d97f855f09dc7b">hidden_size_warning</a></td></tr>
<tr class="separator:a6251ecfed677a65d26d97f855f09dc7b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca3e7f9c9dbd36739756894529b0127b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__attention__unet_1_1FusionAttentionUnet.html#aca3e7f9c9dbd36739756894529b0127b">prune_graph</a></td></tr>
<tr class="separator:aca3e7f9c9dbd36739756894529b0127b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion')"><img src="closed.png" alt="-"/>&#160;Public Attributes inherited from <a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html">onnxruntime.transformers.fusion_base.Fusion</a></td></tr>
<tr class="memitem:a98ba45b80c0c61b8422469cedf9eee5e inherit pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion.html#a98ba45b80c0c61b8422469cedf9eee5e">this_graph_name</a></td></tr>
<tr class="separator:a98ba45b80c0c61b8422469cedf9eee5e inherit pub_attribs_classonnxruntime_1_1transformers_1_1fusion__base_1_1Fusion"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Fuse Attention subgraph of UNet into one Attention node.
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a77d1f8f2c83518f3d3fe387359acd292"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a77d1f8f2c83518f3d3fe387359acd292">&#9670;&nbsp;</a></span>__init__() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a>&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>is_cross_attention</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_packed_qkv</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_packed_kv</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a77d1f8f2c83518f3d3fe387359acd292"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a77d1f8f2c83518f3d3fe387359acd292">&#9670;&nbsp;</a></span>__init__() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1transformers_1_1onnx__model_1_1OnnxModel.html">OnnxModel</a>&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>is_cross_attention</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_packed_qkv</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>enable_packed_kv</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ac881ea0b9dfac33723a6b37a622210e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac881ea0b9dfac33723a6b37a622210e6">&#9670;&nbsp;</a></span>create_attention_node() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.create_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    input (str): input name
    output (str): output name

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="ac881ea0b9dfac33723a6b37a622210e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac881ea0b9dfac33723a6b37a622210e6">&#9670;&nbsp;</a></span>create_attention_node() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.create_attention_node </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    input (str): input name
    output (str): output name

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="aefcc2c4a08f611dda28ebcd2e21e510b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aefcc2c4a08f611dda28ebcd2e21e510b">&#9670;&nbsp;</a></span>create_attention_node_lora() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.create_attention_node_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    input (str): input name
    output (str): output name

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="aefcc2c4a08f611dda28ebcd2e21e510b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aefcc2c4a08f611dda28ebcd2e21e510b">&#9670;&nbsp;</a></span>create_attention_node_lora() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> NodeProto | None onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.create_attention_node_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>q_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>k_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>v_matmul_add</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>num_heads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>hidden_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str&#160;</td>
          <td class="paramname"><em>output</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Create an Attention node.

Args:
    q_matmul (NodeProto): MatMul node in fully connection for Q
    k_matmul (NodeProto): MatMul node in fully connection for K
    v_matmul (NodeProto): MatMul node in fully connection for V
    num_heads (int): number of attention heads. If a model is pruned, it is the number of heads after pruning.
    hidden_size (int): hidden dimension. If a model is pruned, it is the hidden dimension after pruning.
    input (str): input name
    output (str): output name

Returns:
    Union[NodeProto, None]: the node created or None if failed.
</pre> 
</div>
</div>
<a id="abc747f274c53e67f0145c59f76b25807"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abc747f274c53e67f0145c59f76b25807">&#9670;&nbsp;</a></span>fuse() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.fuse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>normalize_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="abc747f274c53e67f0145c59f76b25807"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abc747f274c53e67f0145c59f76b25807">&#9670;&nbsp;</a></span>fuse() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.fuse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>normalize_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1a9953c208b747850da78e9998ca47c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a9953c208b747850da78e9998ca47c8">&#9670;&nbsp;</a></span>fuse_a1111_fp16() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.fuse_a1111_fp16 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>normalize_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Fuse attention of fp16 UNet exported in A1111 (stable diffusion webui) extension</pre> 
</div>
</div>
<a id="a1a9953c208b747850da78e9998ca47c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1a9953c208b747850da78e9998ca47c8">&#9670;&nbsp;</a></span>fuse_a1111_fp16() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.fuse_a1111_fp16 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>normalize_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_name_to_nodes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_name_to_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Fuse attention of fp16 UNet exported in A1111 (stable diffusion webui) extension</pre> 
</div>
</div>
<a id="ae515337d69de76a1669fca0730e54ac3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae515337d69de76a1669fca0730e54ac3">&#9670;&nbsp;</a></span>get_hidden_size() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>layernorm_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect hidden_size from LayerNormalization node.
Args:
    layernorm_node (NodeProto): LayerNormalization node before Q, K and V
Returns:
    int: hidden_size, or 0 if not found
</pre> 
</div>
</div>
<a id="ae515337d69de76a1669fca0730e54ac3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae515337d69de76a1669fca0730e54ac3">&#9670;&nbsp;</a></span>get_hidden_size() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>layernorm_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect hidden_size from LayerNormalization node.
Args:
    layernorm_node (NodeProto): LayerNormalization node before Q, K and V
Returns:
    int: hidden_size, or 0 if not found
</pre> 
</div>
</div>
<a id="a132b8c033f0cb103c7c4d088de7e280f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a132b8c033f0cb103c7c4d088de7e280f">&#9670;&nbsp;</a></span>get_num_heads() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_num_heads </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_torch2</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads from a reshape node.

Args:
    reshape_q (NodeProto): reshape node for Q
    is_torch2 (bool): graph pattern is from PyTorch 2.*
Returns:
    int: num_heads, or 0 if not found
</pre> 
</div>
</div>
<a id="a132b8c033f0cb103c7c4d088de7e280f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a132b8c033f0cb103c7c4d088de7e280f">&#9670;&nbsp;</a></span>get_num_heads() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> int onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_num_heads </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_torch2</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads from a reshape node.

Args:
    reshape_q (NodeProto): reshape node for Q
    is_torch2 (bool): graph pattern is from PyTorch 2.*
Returns:
    int: num_heads, or 0 if not found
</pre> 
</div>
</div>
<a id="a299dc09a193b40e1ff71c3f4ed6487f1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a299dc09a193b40e1ff71c3f4ed6487f1">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_num_heads_and_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>layernorm_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_torch2</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size.

Args:
    reshape_q (NodeProto): reshape node for Q
    is_torch2 (bool): graph pattern is from PyTorch 2.*
    layernorm_node (NodeProto): LayerNormalization node before Q, K, V
Returns:
    Tuple[int, int]: num_heads and hidden_size
</pre> 
</div>
</div>
<a id="a299dc09a193b40e1ff71c3f4ed6487f1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a299dc09a193b40e1ff71c3f4ed6487f1">&#9670;&nbsp;</a></span>get_num_heads_and_hidden_size() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> tuple[int, int] onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.get_num_heads_and_hidden_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>reshape_q</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>layernorm_node</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool &#160;</td>
          <td class="paramname"><em>is_torch2</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Detect num_heads and hidden_size.

Args:
    reshape_q (NodeProto): reshape node for Q
    is_torch2 (bool): graph pattern is from PyTorch 2.*
    layernorm_node (NodeProto): LayerNormalization node before Q, K, V
Returns:
    Tuple[int, int]: num_heads and hidden_size
</pre> 
</div>
</div>
<a id="a4e9008ec2a79f9126e12529e39fc7217"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4e9008ec2a79f9126e12529e39fc7217">&#9670;&nbsp;</a></span>match_lora_path() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_lora_path </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>add_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4e9008ec2a79f9126e12529e39fc7217"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4e9008ec2a79f9126e12529e39fc7217">&#9670;&nbsp;</a></span>match_lora_path() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_lora_path </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">NodeProto&#160;</td>
          <td class="paramname"><em>add_node</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a3c192e064c7bff6cd64bcc66ed14c1dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c192e064c7bff6cd64bcc66ed14c1dc">&#9670;&nbsp;</a></span>match_qkv_a1111() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_a1111 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by A1111 (stable diffusion webui) extension</pre> 
</div>
</div>
<a id="a3c192e064c7bff6cd64bcc66ed14c1dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3c192e064c7bff6cd64bcc66ed14c1dc">&#9670;&nbsp;</a></span>match_qkv_a1111() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_a1111 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by A1111 (stable diffusion webui) extension</pre> 
</div>
</div>
<a id="ac820cf0428704e44a58fa4ab81459c4d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac820cf0428704e44a58fa4ab81459c4d">&#9670;&nbsp;</a></span>match_qkv_torch1() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch1 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 1.*</pre> 
</div>
</div>
<a id="ac820cf0428704e44a58fa4ab81459c4d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac820cf0428704e44a58fa4ab81459c4d">&#9670;&nbsp;</a></span>match_qkv_torch1() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch1 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 1.*</pre> 
</div>
</div>
<a id="a73488e3857bdd5dee28e3ffaac8301ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a73488e3857bdd5dee28e3ffaac8301ad">&#9670;&nbsp;</a></span>match_qkv_torch1_lora() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch1_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 1 that contains LoRA patterns.*</pre> 
</div>
</div>
<a id="a73488e3857bdd5dee28e3ffaac8301ad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a73488e3857bdd5dee28e3ffaac8301ad">&#9670;&nbsp;</a></span>match_qkv_torch1_lora() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch1_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 1 that contains LoRA patterns.*</pre> 
</div>
</div>
<a id="ac1f1fbe676691bbcc37e569ad03304f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac1f1fbe676691bbcc37e569ad03304f0">&#9670;&nbsp;</a></span>match_qkv_torch2() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch2 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 2.*</pre> 
</div>
</div>
<a id="ac1f1fbe676691bbcc37e569ad03304f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac1f1fbe676691bbcc37e569ad03304f0">&#9670;&nbsp;</a></span>match_qkv_torch2() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch2 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 2.*</pre> 
</div>
</div>
<a id="a93fee7a83043882139d09357f3484a45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93fee7a83043882139d09357f3484a45">&#9670;&nbsp;</a></span>match_qkv_torch2_lora() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch2_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 2 that contains LoRA patterns.*</pre> 
</div>
</div>
<a id="a93fee7a83043882139d09357f3484a45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93fee7a83043882139d09357f3484a45">&#9670;&nbsp;</a></span>match_qkv_torch2_lora() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.match_qkv_torch2_lora </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>root_input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_add</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Match Q, K and V paths exported by PyTorch 2 that contains LoRA patterns.*</pre> 
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a66b19f5e0cb7d1f57151155ada4429b2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66b19f5e0cb7d1f57151155ada4429b2">&#9670;&nbsp;</a></span>enable_packed_kv</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.enable_packed_kv</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2ee4e33bf9c748ed41a08e131655d96b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ee4e33bf9c748ed41a08e131655d96b">&#9670;&nbsp;</a></span>enable_packed_qkv</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.enable_packed_qkv</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad1d81dce13ebdaf907ffaa2456eb2713"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad1d81dce13ebdaf907ffaa2456eb2713">&#9670;&nbsp;</a></span>hidden_size</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.hidden_size</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6251ecfed677a65d26d97f855f09dc7b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6251ecfed677a65d26d97f855f09dc7b">&#9670;&nbsp;</a></span>hidden_size_warning</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.hidden_size_warning</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aab0d4c827ae28048492bdcdc929640f9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aab0d4c827ae28048492bdcdc929640f9">&#9670;&nbsp;</a></span>is_cross_attention</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.is_cross_attention</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a21f415524eaf5ddbf63eb7db87d76429"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a21f415524eaf5ddbf63eb7db87d76429">&#9670;&nbsp;</a></span>num_heads</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.num_heads</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="af30ec6385cc7333d918f36d85d169d29"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af30ec6385cc7333d918f36d85d169d29">&#9670;&nbsp;</a></span>num_heads_warning</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.num_heads_warning</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aca3e7f9c9dbd36739756894529b0127b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aca3e7f9c9dbd36739756894529b0127b">&#9670;&nbsp;</a></span>prune_graph</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.transformers.fusion_attention_unet.FusionAttentionUnet.prune_graph</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>chromadb_rest_wrapper/venv/lib/python3.10/site-packages/onnxruntime/transformers/<a class="el" href="chromadb__rest__wrapper_2venv_2lib_2python3_810_2site-packages_2onnxruntime_2transformers_2fusion__attention__unet_8py.html">fusion_attention_unet.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
