<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: openai.resources.completions.AsyncCompletions Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceopenai.html">openai</a></li><li class="navelem"><a class="el" href="namespaceopenai_1_1resources.html">resources</a></li><li class="navelem"><a class="el" href="namespaceopenai_1_1resources_1_1completions.html">completions</a></li><li class="navelem"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html">AsyncCompletions</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">openai.resources.completions.AsyncCompletions Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for openai.resources.completions.AsyncCompletions:</div>
<div class="dyncontent">
<div class="center"><img src="classopenai_1_1resources_1_1completions_1_1AsyncCompletions__inherit__graph.png" border="0" usemap="#aopenai_8resources_8completions_8AsyncCompletions_inherit__map" alt="Inheritance graph"/></div>
<!-- MAP 0 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for openai.resources.completions.AsyncCompletions:</div>
<div class="dyncontent">
<div class="center"><img src="classopenai_1_1resources_1_1completions_1_1AsyncCompletions__coll__graph.png" border="0" usemap="#aopenai_8resources_8completions_8AsyncCompletions_coll__map" alt="Collaboration graph"/></div>
<!-- MAP 1 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aa5d6b2eb58976c22fc186bbd035a1b70"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletionsWithRawResponse.html">AsyncCompletionsWithRawResponse</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#aa5d6b2eb58976c22fc186bbd035a1b70">with_raw_response</a> (self)</td></tr>
<tr class="separator:aa5d6b2eb58976c22fc186bbd035a1b70"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1442b629738685563a2ee1c52921d147"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletionsWithStreamingResponse.html">AsyncCompletionsWithStreamingResponse</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#a1442b629738685563a2ee1c52921d147">with_streaming_response</a> (self)</td></tr>
<tr class="separator:a1442b629738685563a2ee1c52921d147"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad28531baac857a6eb202e08a5a679f80"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#ad28531baac857a6eb202e08a5a679f80">create</a> (self, *Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]] model, Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] prompt, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> best_of=NOT_GIVEN, Optional[bool]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> echo=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> frequency_penalty=NOT_GIVEN, Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logit_bias=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logprobs=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> max_tokens=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> n=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> presence_penalty=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> seed=NOT_GIVEN, Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stop=NOT_GIVEN, Optional[Literal[False]]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream=NOT_GIVEN, Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream_options=NOT_GIVEN, Optional[str]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> suffix=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> temperature=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> top_p=NOT_GIVEN, str|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> user=NOT_GIVEN, Headers|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_headers=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Query|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_query=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Body|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_body=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, float|<a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a>|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> timeout=NOT_GIVEN)</td></tr>
<tr class="separator:ad28531baac857a6eb202e08a5a679f80"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a163c83f280b69aa0af4192344f1c6247"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#a163c83f280b69aa0af4192344f1c6247">create</a> (self, *Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]] model, Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] prompt, Literal[True] stream, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> best_of=NOT_GIVEN, Optional[bool]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> echo=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> frequency_penalty=NOT_GIVEN, Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logit_bias=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logprobs=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> max_tokens=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> n=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> presence_penalty=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> seed=NOT_GIVEN, Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stop=NOT_GIVEN, Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream_options=NOT_GIVEN, Optional[str]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> suffix=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> temperature=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> top_p=NOT_GIVEN, str|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> user=NOT_GIVEN, Headers|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_headers=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Query|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_query=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Body|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_body=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, float|<a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a>|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> timeout=NOT_GIVEN)</td></tr>
<tr class="separator:a163c83f280b69aa0af4192344f1c6247"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac72f0e892065d2816338f5338ffd9e4d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>|<a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#ac72f0e892065d2816338f5338ffd9e4d">create</a> (self, *Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]] model, Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] prompt, bool stream, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> best_of=NOT_GIVEN, Optional[bool]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> echo=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> frequency_penalty=NOT_GIVEN, Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logit_bias=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logprobs=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> max_tokens=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> n=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> presence_penalty=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> seed=NOT_GIVEN, Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stop=NOT_GIVEN, Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream_options=NOT_GIVEN, Optional[str]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> suffix=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> temperature=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> top_p=NOT_GIVEN, str|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> user=NOT_GIVEN, Headers|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_headers=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Query|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_query=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Body|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_body=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, float|<a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a>|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> timeout=NOT_GIVEN)</td></tr>
<tr class="separator:ac72f0e892065d2816338f5338ffd9e4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57d350b00447aeea5ea7df6c3424529f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>|<a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletions.html#a57d350b00447aeea5ea7df6c3424529f">create</a> (self, *Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]] model, Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] prompt, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> best_of=NOT_GIVEN, Optional[bool]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> echo=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> frequency_penalty=NOT_GIVEN, Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logit_bias=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> logprobs=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> max_tokens=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> n=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> presence_penalty=NOT_GIVEN, Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> seed=NOT_GIVEN, Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stop=NOT_GIVEN, Optional[Literal[False]]|Literal[True]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream=NOT_GIVEN, Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> stream_options=NOT_GIVEN, Optional[str]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> suffix=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> temperature=NOT_GIVEN, Optional[float]|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> top_p=NOT_GIVEN, str|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> user=NOT_GIVEN, Headers|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_headers=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Query|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_query=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, Body|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> extra_body=<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>, float|<a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a>|<a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>|<a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> timeout=NOT_GIVEN)</td></tr>
<tr class="separator:a57d350b00447aeea5ea7df6c3424529f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classopenai_1_1__resource_1_1AsyncAPIResource"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classopenai_1_1__resource_1_1AsyncAPIResource')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classopenai_1_1__resource_1_1AsyncAPIResource.html">openai._resource.AsyncAPIResource</a></td></tr>
<tr class="memitem:aa1372be933fa686115985c33d605dad6 inherit pub_methods_classopenai_1_1__resource_1_1AsyncAPIResource"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classopenai_1_1__resource_1_1AsyncAPIResource.html#aa1372be933fa686115985c33d605dad6">__init__</a> (self, <a class="el" href="classopenai_1_1__client_1_1AsyncOpenAI.html">AsyncOpenAI</a> client)</td></tr>
<tr class="separator:aa1372be933fa686115985c33d605dad6 inherit pub_methods_classopenai_1_1__resource_1_1AsyncAPIResource"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ac72f0e892065d2816338f5338ffd9e4d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac72f0e892065d2816338f5338ffd9e4d">&#9670;&nbsp;</a></span>create() <span class="overload">[1/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a> | <a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>] openai.resources.completions.AsyncCompletions.create </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]]&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]&#160;</td>
          <td class="paramname"><em>prompt</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>stream</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>best_of</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[bool] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>echo</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>frequency_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logit_bias</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logprobs</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>max_tokens</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>n</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>presence_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>seed</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stop</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream_options</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>suffix</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>temperature</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>top_p</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>user</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Headers | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_headers</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Query | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_query</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Body | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_body</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | <a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a> | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>NOT_GIVEN</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Creates a completion for the provided prompt and parameters.

Args:
  model: ID of the model to use. You can use the
      [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      see all of your available models, or see our
      [Model overview](https://platform.openai.com/docs/models) for descriptions of
      them.

  prompt: The prompt(s) to generate completions for, encoded as a string, array of
      strings, array of tokens, or array of token arrays.

      Note that &lt;|endoftext|&gt; is the document separator that the model sees during
      training, so if a prompt is not specified the model will generate as if from the
      beginning of a new document.

  stream: Whether to stream back partial progress. If set, tokens will be sent as
      data-only
      [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
      as they become available, with the stream terminated by a `data: [DONE]`
      message.
      [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).

  best_of: Generates `best_of` completions server-side and returns the "best" (the one with
      the highest log probability per token). Results cannot be streamed.

      When used with `n`, `best_of` controls the number of candidate completions and
      `n` specifies how many to return – `best_of` must be greater than `n`.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  echo: Echo back the prompt in addition to the completion

  frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
      existing frequency in the text so far, decreasing the model's likelihood to
      repeat the same line verbatim.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  logit_bias: Modify the likelihood of specified tokens appearing in the completion.

      Accepts a JSON object that maps tokens (specified by their token ID in the GPT
      tokenizer) to an associated bias value from -100 to 100. You can use this
      [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
      Mathematically, the bias is added to the logits generated by the model prior to
      sampling. The exact effect will vary per model, but values between -1 and 1
      should decrease or increase likelihood of selection; values like -100 or 100
      should result in a ban or exclusive selection of the relevant token.

      As an example, you can pass `{"50256": -100}` to prevent the &lt;|endoftext|&gt; token
      from being generated.

  logprobs: Include the log probabilities on the `logprobs` most likely output tokens, as
      well the chosen tokens. For example, if `logprobs` is 5, the API will return a
      list of the 5 most likely tokens. The API will always return the `logprob` of
      the sampled token, so there may be up to `logprobs+1` elements in the response.

      The maximum value for `logprobs` is 5.

  max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the
      completion.

      The token count of your prompt plus `max_tokens` cannot exceed the model's
      context length.
      [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      for counting tokens.

  n: How many completions to generate for each prompt.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
      whether they appear in the text so far, increasing the model's likelihood to
      talk about new topics.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  seed: If specified, our system will make a best effort to sample deterministically,
      such that repeated requests with the same `seed` and parameters should return
      the same result.

      Determinism is not guaranteed, and you should refer to the `system_fingerprint`
      response parameter to monitor changes in the backend.

  stop: Up to 4 sequences where the API will stop generating further tokens. The
      returned text will not contain the stop sequence.

  stream_options: Options for streaming response. Only set this when you set `stream: true`.

  suffix: The suffix that comes after a completion of inserted text.

      This parameter is only supported for `gpt-3.5-turbo-instruct`.

  temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      make the output more random, while lower values like 0.2 will make it more
      focused and deterministic.

      We generally recommend altering this or `top_p` but not both.

  top_p: An alternative to sampling with temperature, called nucleus sampling, where the
      model considers the results of the tokens with top_p probability mass. So 0.1
      means only the tokens comprising the top 10% probability mass are considered.

      We generally recommend altering this or `temperature` but not both.

  user: A unique identifier representing your end-user, which can help OpenAI to monitor
      and detect abuse.
      [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).

  extra_headers: Send extra headers

  extra_query: Add additional query parameters to the request

  extra_body: Add additional JSON properties to the request

  timeout: Override the client-level default timeout for this request, in seconds
</pre> 
</div>
</div>
<a id="a163c83f280b69aa0af4192344f1c6247"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a163c83f280b69aa0af4192344f1c6247">&#9670;&nbsp;</a></span>create() <span class="overload">[2/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>] openai.resources.completions.AsyncCompletions.create </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]]&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]&#160;</td>
          <td class="paramname"><em>prompt</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Literal[True]&#160;</td>
          <td class="paramname"><em>stream</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>best_of</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[bool] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>echo</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>frequency_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logit_bias</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logprobs</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>max_tokens</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>n</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>presence_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>seed</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stop</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream_options</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>suffix</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>temperature</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>top_p</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>user</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Headers | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_headers</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Query | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_query</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Body | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_body</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | <a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a> | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>NOT_GIVEN</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Creates a completion for the provided prompt and parameters.

Args:
  model: ID of the model to use. You can use the
      [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      see all of your available models, or see our
      [Model overview](https://platform.openai.com/docs/models) for descriptions of
      them.

  prompt: The prompt(s) to generate completions for, encoded as a string, array of
      strings, array of tokens, or array of token arrays.

      Note that &lt;|endoftext|&gt; is the document separator that the model sees during
      training, so if a prompt is not specified the model will generate as if from the
      beginning of a new document.

  stream: Whether to stream back partial progress. If set, tokens will be sent as
      data-only
      [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
      as they become available, with the stream terminated by a `data: [DONE]`
      message.
      [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).

  best_of: Generates `best_of` completions server-side and returns the "best" (the one with
      the highest log probability per token). Results cannot be streamed.

      When used with `n`, `best_of` controls the number of candidate completions and
      `n` specifies how many to return – `best_of` must be greater than `n`.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  echo: Echo back the prompt in addition to the completion

  frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
      existing frequency in the text so far, decreasing the model's likelihood to
      repeat the same line verbatim.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  logit_bias: Modify the likelihood of specified tokens appearing in the completion.

      Accepts a JSON object that maps tokens (specified by their token ID in the GPT
      tokenizer) to an associated bias value from -100 to 100. You can use this
      [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
      Mathematically, the bias is added to the logits generated by the model prior to
      sampling. The exact effect will vary per model, but values between -1 and 1
      should decrease or increase likelihood of selection; values like -100 or 100
      should result in a ban or exclusive selection of the relevant token.

      As an example, you can pass `{"50256": -100}` to prevent the &lt;|endoftext|&gt; token
      from being generated.

  logprobs: Include the log probabilities on the `logprobs` most likely output tokens, as
      well the chosen tokens. For example, if `logprobs` is 5, the API will return a
      list of the 5 most likely tokens. The API will always return the `logprob` of
      the sampled token, so there may be up to `logprobs+1` elements in the response.

      The maximum value for `logprobs` is 5.

  max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the
      completion.

      The token count of your prompt plus `max_tokens` cannot exceed the model's
      context length.
      [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      for counting tokens.

  n: How many completions to generate for each prompt.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
      whether they appear in the text so far, increasing the model's likelihood to
      talk about new topics.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  seed: If specified, our system will make a best effort to sample deterministically,
      such that repeated requests with the same `seed` and parameters should return
      the same result.

      Determinism is not guaranteed, and you should refer to the `system_fingerprint`
      response parameter to monitor changes in the backend.

  stop: Up to 4 sequences where the API will stop generating further tokens. The
      returned text will not contain the stop sequence.

  stream_options: Options for streaming response. Only set this when you set `stream: true`.

  suffix: The suffix that comes after a completion of inserted text.

      This parameter is only supported for `gpt-3.5-turbo-instruct`.

  temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      make the output more random, while lower values like 0.2 will make it more
      focused and deterministic.

      We generally recommend altering this or `top_p` but not both.

  top_p: An alternative to sampling with temperature, called nucleus sampling, where the
      model considers the results of the tokens with top_p probability mass. So 0.1
      means only the tokens comprising the top 10% probability mass are considered.

      We generally recommend altering this or `temperature` but not both.

  user: A unique identifier representing your end-user, which can help OpenAI to monitor
      and detect abuse.
      [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).

  extra_headers: Send extra headers

  extra_query: Add additional query parameters to the request

  extra_body: Add additional JSON properties to the request

  timeout: Override the client-level default timeout for this request, in seconds
</pre> 
</div>
</div>
<a id="a57d350b00447aeea5ea7df6c3424529f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a57d350b00447aeea5ea7df6c3424529f">&#9670;&nbsp;</a></span>create() <span class="overload">[3/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a> | <a class="el" href="classopenai_1_1__streaming_1_1AsyncStream.html">AsyncStream</a>[<a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a>] openai.resources.completions.AsyncCompletions.create </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]]&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]&#160;</td>
          <td class="paramname"><em>prompt</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>best_of</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[bool] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>echo</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>frequency_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logit_bias</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logprobs</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>max_tokens</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>n</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>presence_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>seed</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stop</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Literal[False]] | Literal[True] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream_options</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>suffix</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>temperature</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>top_p</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>user</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Headers | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_headers</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Query | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_query</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Body | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_body</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | <a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a> | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>NOT_GIVEN</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad28531baac857a6eb202e08a5a679f80"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad28531baac857a6eb202e08a5a679f80">&#9670;&nbsp;</a></span>create() <span class="overload">[4/4]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1types_1_1completion_1_1Completion.html">Completion</a> openai.resources.completions.AsyncCompletions.create </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Union[str, Literal[&quot;gpt-3.5-turbo-instruct&quot;, &quot;davinci-002&quot;, &quot;babbage-002&quot;]]&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[str, List[str], Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>], Iterable[Iterable[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>]&#160;</td>
          <td class="paramname"><em>prompt</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>best_of</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[bool] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>echo</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>frequency_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Dict[str, <a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>]] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logit_bias</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>logprobs</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>max_tokens</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>n</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>presence_penalty</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="namespaceopenai.html#a83d16b3bfa0c66f7bbe2ca8ca7f774b5">int</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>seed</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Union[Optional[str], List[str], <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stop</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[Literal[False]] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[<a class="el" href="classopenai_1_1types_1_1chat_1_1chat__completion__stream__options__param_1_1ChatCompletionStreamOptionsParam.html">ChatCompletionStreamOptionsParam</a>] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>stream_options</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[str] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>suffix</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>temperature</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>top_p</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>user</em> = <code>NOT_GIVEN</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Headers | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_headers</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Query | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_query</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Body | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> &#160;</td>
          <td class="paramname"><em>extra_body</em> = <code><a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float | <a class="el" href="classhttpx_1_1__config_1_1Timeout.html">httpx.Timeout</a> | <a class="el" href="namespaceopenai.html#aeb88e19585df4a9b05362135363f4739">None</a> | <a class="el" href="classopenai_1_1__types_1_1NotGiven.html">NotGiven</a> &#160;</td>
          <td class="paramname"><em>timeout</em> = <code>NOT_GIVEN</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Creates a completion for the provided prompt and parameters.

Args:
  model: ID of the model to use. You can use the
      [List models](https://platform.openai.com/docs/api-reference/models/list) API to
      see all of your available models, or see our
      [Model overview](https://platform.openai.com/docs/models) for descriptions of
      them.

  prompt: The prompt(s) to generate completions for, encoded as a string, array of
      strings, array of tokens, or array of token arrays.

      Note that &lt;|endoftext|&gt; is the document separator that the model sees during
      training, so if a prompt is not specified the model will generate as if from the
      beginning of a new document.

  best_of: Generates `best_of` completions server-side and returns the "best" (the one with
      the highest log probability per token). Results cannot be streamed.

      When used with `n`, `best_of` controls the number of candidate completions and
      `n` specifies how many to return – `best_of` must be greater than `n`.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  echo: Echo back the prompt in addition to the completion

  frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their
      existing frequency in the text so far, decreasing the model's likelihood to
      repeat the same line verbatim.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  logit_bias: Modify the likelihood of specified tokens appearing in the completion.

      Accepts a JSON object that maps tokens (specified by their token ID in the GPT
      tokenizer) to an associated bias value from -100 to 100. You can use this
      [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs.
      Mathematically, the bias is added to the logits generated by the model prior to
      sampling. The exact effect will vary per model, but values between -1 and 1
      should decrease or increase likelihood of selection; values like -100 or 100
      should result in a ban or exclusive selection of the relevant token.

      As an example, you can pass `{"50256": -100}` to prevent the &lt;|endoftext|&gt; token
      from being generated.

  logprobs: Include the log probabilities on the `logprobs` most likely output tokens, as
      well the chosen tokens. For example, if `logprobs` is 5, the API will return a
      list of the 5 most likely tokens. The API will always return the `logprob` of
      the sampled token, so there may be up to `logprobs+1` elements in the response.

      The maximum value for `logprobs` is 5.

  max_tokens: The maximum number of [tokens](/tokenizer) that can be generated in the
      completion.

      The token count of your prompt plus `max_tokens` cannot exceed the model's
      context length.
      [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
      for counting tokens.

  n: How many completions to generate for each prompt.

      **Note:** Because this parameter generates many completions, it can quickly
      consume your token quota. Use carefully and ensure that you have reasonable
      settings for `max_tokens` and `stop`.

  presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on
      whether they appear in the text so far, increasing the model's likelihood to
      talk about new topics.

      [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation)

  seed: If specified, our system will make a best effort to sample deterministically,
      such that repeated requests with the same `seed` and parameters should return
      the same result.

      Determinism is not guaranteed, and you should refer to the `system_fingerprint`
      response parameter to monitor changes in the backend.

  stop: Up to 4 sequences where the API will stop generating further tokens. The
      returned text will not contain the stop sequence.

  stream: Whether to stream back partial progress. If set, tokens will be sent as
      data-only
      [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
      as they become available, with the stream terminated by a `data: [DONE]`
      message.
      [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).

  stream_options: Options for streaming response. Only set this when you set `stream: true`.

  suffix: The suffix that comes after a completion of inserted text.

      This parameter is only supported for `gpt-3.5-turbo-instruct`.

  temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
      make the output more random, while lower values like 0.2 will make it more
      focused and deterministic.

      We generally recommend altering this or `top_p` but not both.

  top_p: An alternative to sampling with temperature, called nucleus sampling, where the
      model considers the results of the tokens with top_p probability mass. So 0.1
      means only the tokens comprising the top 10% probability mass are considered.

      We generally recommend altering this or `temperature` but not both.

  user: A unique identifier representing your end-user, which can help OpenAI to monitor
      and detect abuse.
      [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).

  extra_headers: Send extra headers

  extra_query: Add additional query parameters to the request

  extra_body: Add additional JSON properties to the request

  timeout: Override the client-level default timeout for this request, in seconds
</pre> 
</div>
</div>
<a id="aa5d6b2eb58976c22fc186bbd035a1b70"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa5d6b2eb58976c22fc186bbd035a1b70">&#9670;&nbsp;</a></span>with_raw_response()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletionsWithRawResponse.html">AsyncCompletionsWithRawResponse</a> openai.resources.completions.AsyncCompletions.with_raw_response </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">This property can be used as a prefix for any HTTP method call to return
the raw response object instead of the parsed content.

For more information, see https://www.github.com/openai/openai-python#accessing-raw-response-data-eg-headers
</pre> 
</div>
</div>
<a id="a1442b629738685563a2ee1c52921d147"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1442b629738685563a2ee1c52921d147">&#9670;&nbsp;</a></span>with_streaming_response()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> <a class="el" href="classopenai_1_1resources_1_1completions_1_1AsyncCompletionsWithStreamingResponse.html">AsyncCompletionsWithStreamingResponse</a> openai.resources.completions.AsyncCompletions.with_streaming_response </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">An alternative to `.with_raw_response` that doesn't eagerly read the response body.

For more information, see https://www.github.com/openai/openai-python#with_streaming_response
</pre> 
</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>py3_env/lib/python3.10/site-packages/openai/resources/<a class="el" href="resources_2completions_8py.html">completions.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
