<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NAO Virtual Storyteller: onnxruntime.quantization.quantize.StaticQuantConfig Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">NAO Virtual Storyteller
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespaceonnxruntime.html">onnxruntime</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1quantization.html">quantization</a></li><li class="navelem"><a class="el" href="namespaceonnxruntime_1_1quantization_1_1quantize.html">quantize</a></li><li class="navelem"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html">StaticQuantConfig</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">onnxruntime.quantization.quantize.StaticQuantConfig Class Reference</div>  </div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for onnxruntime.quantization.quantize.StaticQuantConfig:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig__inherit__graph.png" border="0" usemap="#aonnxruntime_8quantization_8quantize_8StaticQuantConfig_inherit__map" alt="Inheritance graph"/></div>
<!-- MAP 0 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for onnxruntime.quantization.quantize.StaticQuantConfig:</div>
<div class="dyncontent">
<div class="center"><img src="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig__coll__graph.png" border="0" usemap="#aonnxruntime_8quantization_8quantize_8StaticQuantConfig_coll__map" alt="Collaboration graph"/></div>
<!-- MAP 1 -->
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ad281e1b0d6de36061c494bff207277e8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ad281e1b0d6de36061c494bff207277e8">__init__</a> (self, <a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationDataReader.html">CalibrationDataReader</a> <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a8227375afa0e4320d8384241b4bde833">calibration_data_reader</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ae47023feec39104444adf5b125c82b3e">calibrate_method</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationMethod.html#a554f8473db14001a78e35366aa7f704e">CalibrationMethod.MinMax</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ab5a24155d9eb0286f3ab66dc8a61a569">quant_format</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantFormat.html#a8f56b75bb0d98dc315e8923d4a8610ce">QuantFormat.QDQ</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a2ad6649b7499dfdab64a7a5595473fcd">activation_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#afb0b88e3b7590aa5b5e3be614bd2fda6">weight_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#af70533c2b84318ee79ce28e5ccae3d8e">op_types_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac41b0e41f63a67dab855b11d8f27a162">nodes_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac82c05ca3f09d3a3cbb1b354e4e07c3f">nodes_to_exclude</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aff78b561a17efe745d622c401416301d">per_channel</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ae329aab8f25e201a9f68194d151957a1">reduce_range</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aa51e1ad5482890655a09372a45a984d0">use_external_data_format</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a97a9b31e9517cc6a9f6418a9d8caec0f">calibration_providers</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a66161a0a298affd760e120417fb45f09">extra_options</a>=None)</td></tr>
<tr class="separator:ad281e1b0d6de36061c494bff207277e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad281e1b0d6de36061c494bff207277e8"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ad281e1b0d6de36061c494bff207277e8">__init__</a> (self, <a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationDataReader.html">CalibrationDataReader</a> <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a8227375afa0e4320d8384241b4bde833">calibration_data_reader</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ae47023feec39104444adf5b125c82b3e">calibrate_method</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationMethod.html#a554f8473db14001a78e35366aa7f704e">CalibrationMethod.MinMax</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ab5a24155d9eb0286f3ab66dc8a61a569">quant_format</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantFormat.html#a8f56b75bb0d98dc315e8923d4a8610ce">QuantFormat.QDQ</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a2ad6649b7499dfdab64a7a5595473fcd">activation_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#afb0b88e3b7590aa5b5e3be614bd2fda6">weight_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#af70533c2b84318ee79ce28e5ccae3d8e">op_types_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac41b0e41f63a67dab855b11d8f27a162">nodes_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac82c05ca3f09d3a3cbb1b354e4e07c3f">nodes_to_exclude</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aff78b561a17efe745d622c401416301d">per_channel</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ae329aab8f25e201a9f68194d151957a1">reduce_range</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aa51e1ad5482890655a09372a45a984d0">use_external_data_format</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a97a9b31e9517cc6a9f6418a9d8caec0f">calibration_providers</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a66161a0a298affd760e120417fb45f09">extra_options</a>=None)</td></tr>
<tr class="separator:ad281e1b0d6de36061c494bff207277e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html">onnxruntime.quantization.quantize.QuantConfig</a></td></tr>
<tr class="memitem:a021d8e0605cd5ab778c75e9470cf5b8c inherit pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a021d8e0605cd5ab778c75e9470cf5b8c">__init__</a> (self, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a2ad6649b7499dfdab64a7a5595473fcd">activation_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a8fa19b96e148140d1b46328340833fbe">QuantType.QUInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#afb0b88e3b7590aa5b5e3be614bd2fda6">weight_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#af70533c2b84318ee79ce28e5ccae3d8e">op_types_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac41b0e41f63a67dab855b11d8f27a162">nodes_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac82c05ca3f09d3a3cbb1b354e4e07c3f">nodes_to_exclude</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aff78b561a17efe745d622c401416301d">per_channel</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ae329aab8f25e201a9f68194d151957a1">reduce_range</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aa51e1ad5482890655a09372a45a984d0">use_external_data_format</a>=False)</td></tr>
<tr class="separator:a021d8e0605cd5ab778c75e9470cf5b8c inherit pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a021d8e0605cd5ab778c75e9470cf5b8c inherit pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">def&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a021d8e0605cd5ab778c75e9470cf5b8c">__init__</a> (self, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a2ad6649b7499dfdab64a7a5595473fcd">activation_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a8fa19b96e148140d1b46328340833fbe">QuantType.QUInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#afb0b88e3b7590aa5b5e3be614bd2fda6">weight_type</a>=<a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a>, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#af70533c2b84318ee79ce28e5ccae3d8e">op_types_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac41b0e41f63a67dab855b11d8f27a162">nodes_to_quantize</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac82c05ca3f09d3a3cbb1b354e4e07c3f">nodes_to_exclude</a>=None, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aff78b561a17efe745d622c401416301d">per_channel</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ae329aab8f25e201a9f68194d151957a1">reduce_range</a>=False, <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aa51e1ad5482890655a09372a45a984d0">use_external_data_format</a>=False)</td></tr>
<tr class="separator:a021d8e0605cd5ab778c75e9470cf5b8c inherit pub_methods_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a8227375afa0e4320d8384241b4bde833"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a8227375afa0e4320d8384241b4bde833">calibration_data_reader</a></td></tr>
<tr class="separator:a8227375afa0e4320d8384241b4bde833"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae47023feec39104444adf5b125c82b3e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ae47023feec39104444adf5b125c82b3e">calibrate_method</a></td></tr>
<tr class="separator:ae47023feec39104444adf5b125c82b3e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5a24155d9eb0286f3ab66dc8a61a569"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#ab5a24155d9eb0286f3ab66dc8a61a569">quant_format</a></td></tr>
<tr class="separator:ab5a24155d9eb0286f3ab66dc8a61a569"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a97a9b31e9517cc6a9f6418a9d8caec0f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a97a9b31e9517cc6a9f6418a9d8caec0f">calibration_providers</a></td></tr>
<tr class="separator:a97a9b31e9517cc6a9f6418a9d8caec0f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66161a0a298affd760e120417fb45f09"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1StaticQuantConfig.html#a66161a0a298affd760e120417fb45f09">extra_options</a></td></tr>
<tr class="separator:a66161a0a298affd760e120417fb45f09"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig')"><img src="closed.png" alt="-"/>&#160;Public Attributes inherited from <a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html">onnxruntime.quantization.quantize.QuantConfig</a></td></tr>
<tr class="memitem:af70533c2b84318ee79ce28e5ccae3d8e inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#af70533c2b84318ee79ce28e5ccae3d8e">op_types_to_quantize</a></td></tr>
<tr class="separator:af70533c2b84318ee79ce28e5ccae3d8e inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff78b561a17efe745d622c401416301d inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aff78b561a17efe745d622c401416301d">per_channel</a></td></tr>
<tr class="separator:aff78b561a17efe745d622c401416301d inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae329aab8f25e201a9f68194d151957a1 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ae329aab8f25e201a9f68194d151957a1">reduce_range</a></td></tr>
<tr class="separator:ae329aab8f25e201a9f68194d151957a1 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb0b88e3b7590aa5b5e3be614bd2fda6 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#afb0b88e3b7590aa5b5e3be614bd2fda6">weight_type</a></td></tr>
<tr class="separator:afb0b88e3b7590aa5b5e3be614bd2fda6 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ad6649b7499dfdab64a7a5595473fcd inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#a2ad6649b7499dfdab64a7a5595473fcd">activation_type</a></td></tr>
<tr class="separator:a2ad6649b7499dfdab64a7a5595473fcd inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac41b0e41f63a67dab855b11d8f27a162 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac41b0e41f63a67dab855b11d8f27a162">nodes_to_quantize</a></td></tr>
<tr class="separator:ac41b0e41f63a67dab855b11d8f27a162 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac82c05ca3f09d3a3cbb1b354e4e07c3f inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#ac82c05ca3f09d3a3cbb1b354e4e07c3f">nodes_to_exclude</a></td></tr>
<tr class="separator:ac82c05ca3f09d3a3cbb1b354e4e07c3f inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa51e1ad5482890655a09372a45a984d0 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig.html#aa51e1ad5482890655a09372a45a984d0">use_external_data_format</a></td></tr>
<tr class="separator:aa51e1ad5482890655a09372a45a984d0 inherit pub_attribs_classonnxruntime_1_1quantization_1_1quantize_1_1QuantConfig"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="ad281e1b0d6de36061c494bff207277e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad281e1b0d6de36061c494bff207277e8">&#9670;&nbsp;</a></span>__init__() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.quantization.quantize.StaticQuantConfig.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationDataReader.html">CalibrationDataReader</a>&#160;</td>
          <td class="paramname"><em>calibration_data_reader</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>calibrate_method</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationMethod.html#a554f8473db14001a78e35366aa7f704e">CalibrationMethod.MinMax</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>quant_format</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantFormat.html#a8f56b75bb0d98dc315e8923d4a8610ce">QuantFormat.QDQ</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>activation_type</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>weight_type</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>op_types_to_quantize</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>nodes_to_quantize</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>nodes_to_exclude</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>per_channel</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>reduce_range</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>use_external_data_format</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>calibration_providers</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>extra_options</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">This is the derived class for static Quantize Configuration

Args:
    calibration_data_reader:
        a calibration data reader. It enumerates calibration data and generates inputs for the original model.
    calibrate_method:
        Current calibration methods supported are MinMax, Entropy and Percentile.
    quant_format: QuantFormat{QOperator, QDQ}.
        QOperator format quantizes the model with quantized operators directly.
        QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
    calibration_providers: Execution providers to run the session during calibration. Default is None which uses
        [ "CPUExecutionProvider" ].
    extra_options:
        key value pair dictionary for various options in different case. Current used:
            extra.Sigmoid.nnapi = True/False  (Default is False)
            ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).
            WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).
            EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.
                                          Dyanmic mode currently is supported. Will support more in future.
            ForceQuantizeNoInputCheck = True/False :
                By default, some latent operators like maxpool, transpose, do not quantize if their input is not
                quantized already. Setting to True to force such operator always quantize input and so generate
                quantized output. Also the True behavior could be disabled per node using the nodes_to_exclude.
            MatMulConstBOnly = True/False:
                Default is False for static mode. If enabled, only MatMul with const B will be quantized.
            AddQDQPairToWeight = True/False :
                Default is False which quantizes floating-point weight and feeds it to solely inserted
                DeQuantizeLinear node. If True, it remains floating-point weight and inserts both
                QuantizeLinear/DeQuantizeLinear nodes to weight.
            OpTypesToExcludeOutputQuantization = list of op type :
                Default is []. If any op type is specified, it won't quantize the output of ops with this
                specific op types.
            DedicatedQDQPair = True/False :
                Default is False. When inserting QDQ pair, multiple nodes can share a single QDQ pair as their
                inputs. If True, it will create identical and dedicated QDQ pair for each node.
            QDQOpTypePerChannelSupportToAxis = dictionary :
                Default is {}. Set channel axis for specific op type, for example: {'MatMul': 1}, and it's
                effective only when per channel quantization is supported and per_channel is True. If specific
                op type supports per channel quantization but not explicitly specified with channel axis,
                default channel axis will be used.
            CalibTensorRangeSymmetric = True/False :
                Default is False. If enabled, the final range of tensor during calibration will be explicitly
                set to symmetric to central point "0".
            CalibMovingAverage = True/False :
                Default is False. If enabled, the moving average of the minimum and maximum values will be
                computed when the calibration method selected is MinMax.
            CalibMovingAverageConstant = float :
                Default is 0.01. Constant smoothing factor to use when computing the moving average of the
                minimum and maximum values. Effective only when the calibration method selected is MinMax and
                when CalibMovingAverage is set to True.
            QuantizeBias = True/False :
                Default is True which quantizes floating-point biases and it solely inserts
                a DeQuantizeLinear node. If False, it remains floating-point bias and does not insert
                any quantization nodes associated with biases.
                This extra option is only effective when quant_format is QuantFormat.QDQ.
            SmoothQuant = True/False :
                Default is False. If enabled, SmoothQuant algorithm will be applied before quantization to do
                fake input channel quantization.
            SmoothQuantAlpha = float :
                Default is 0.5. It only works if SmoothQuant is True. It controls the difficulty of weight
                and activation quantization. A larger alpha value could be used on models with more significant
                activation outliers to migrate more quantization difficulty to weights.
            SmoothQuantFolding = True/False :
                Default is True. It only works if SmoothQuant is True. If enabled, inserted Mul ops during
                SmoothQuant will be folded into the previous op if the previous op is foldable.
            UseQDQContribOps = True/False :
                Default is False. If enabled, the inserted QuantizeLinear and DequantizeLinear ops will have the
                `com.microsoft` domain, which forces use of ONNX Runtime's QuantizeLinear and DequantizeLinear
                contrib op implementations. The contrib op implementations may support features not standardized
                into the ONNX specification (e.g., 16-bit quantization types).
            MinimumRealRange = float|None :
                Default is None. If set to a floating-point value, the calculation of the quantization parameters
                (i.e., scale and zero point) will enforce a minimum range between rmin and rmax. If (rmax-rmin)
                is less than the specified minimum range, rmax will be set to rmin + MinimumRealRange. This is
                necessary for EPs like QNN that require a minimum floating-point range when determining
                quantization parameters.
            TensorQuantOverrides = dictionary :
                Default is {}. Set tensor quantization overrides. The key is a tensor name and the value is a
                list of dictionaries. For per-tensor quantization, the list contains a single dictionary. For
                per-channel quantization, the list contains a dictionary for each channel in the tensor.
                Each dictionary contains optional overrides with the following keys and values.
                    'quant_type' = QuantType : The tensor's quantization data type.
                    'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.
                    'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.
                    'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also
                                               set `scale` or `zero_point`.
                    'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also
                                               set `scale` or `zero_point`.
                    'rmax' = Float           : Override the maximum real tensor value in calibration data.
                                               Invalid if also set `scale` or `zero_point`.
                    'rmin' = Float           : Override the minimum real tensor value in calibration data.
                                               Invalid if also set `scale` or `zero_point`.
            QDQKeepRemovableActivations = True/False:
                Default is False. If true, "removable" activations (e.g., Clip or Relu) will not be removed, and
                will be explicitly represented in the QDQ model. If false, these activations are automatically
                removed if activations are asymmetrically quantized. Keeping these activations is necessary if
                optimizations or EP transformations will later remove QuantizeLinear/DequantizeLinear
                operators from the model.
            QDQDisableWeightAdjustForInt32Bias = True/False:
                Default is False. If true, QDQ quantizer will not adjust the weight's scale when the bias
                has a scale (input_scale * weight_scale) that is too small.
    execution_provider : A enum indicates the Execution Provider such as: CPU, TRT, NNAPI, SNE, etc.
Raises:
    ValueError: Raise ValueError if execution provider is unknown
</pre> 
</div>
</div>
<a id="ad281e1b0d6de36061c494bff207277e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad281e1b0d6de36061c494bff207277e8">&#9670;&nbsp;</a></span>__init__() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">def onnxruntime.quantization.quantize.StaticQuantConfig.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationDataReader.html">CalibrationDataReader</a>&#160;</td>
          <td class="paramname"><em>calibration_data_reader</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>calibrate_method</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1calibrate_1_1CalibrationMethod.html#a554f8473db14001a78e35366aa7f704e">CalibrationMethod.MinMax</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>quant_format</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantFormat.html#a8f56b75bb0d98dc315e8923d4a8610ce">QuantFormat.QDQ</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>activation_type</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>weight_type</em> = <code><a class="el" href="classonnxruntime_1_1quantization_1_1quant__utils_1_1QuantType.html#a28fa5fa06ceb77e808d6771f97e3e85d">QuantType.QInt8</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>op_types_to_quantize</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>nodes_to_quantize</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>nodes_to_exclude</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>per_channel</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>reduce_range</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>use_external_data_format</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>calibration_providers</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>extra_options</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">This is the derived class for static Quantize Configuration

Args:
    calibration_data_reader:
        a calibration data reader. It enumerates calibration data and generates inputs for the original model.
    calibrate_method:
        Current calibration methods supported are MinMax, Entropy and Percentile.
    quant_format: QuantFormat{QOperator, QDQ}.
        QOperator format quantizes the model with quantized operators directly.
        QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.
    calibration_providers: Execution providers to run the session during calibration. Default is None which uses
        [ "CPUExecutionProvider" ].
    extra_options:
        key value pair dictionary for various options in different case. Current used:
            extra.Sigmoid.nnapi = True/False  (Default is False)
            ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).
            WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).
            EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.
                                          Dyanmic mode currently is supported. Will support more in future.
            ForceQuantizeNoInputCheck = True/False :
                By default, some latent operators like maxpool, transpose, do not quantize if their input is not
                quantized already. Setting to True to force such operator always quantize input and so generate
                quantized output. Also the True behavior could be disabled per node using the nodes_to_exclude.
            MatMulConstBOnly = True/False:
                Default is False for static mode. If enabled, only MatMul with const B will be quantized.
            AddQDQPairToWeight = True/False :
                Default is False which quantizes floating-point weight and feeds it to solely inserted
                DeQuantizeLinear node. If True, it remains floating-point weight and inserts both
                QuantizeLinear/DeQuantizeLinear nodes to weight.
            OpTypesToExcludeOutputQuantization = list of op type :
                Default is []. If any op type is specified, it won't quantize the output of ops with this
                specific op types.
            DedicatedQDQPair = True/False :
                Default is False. When inserting QDQ pair, multiple nodes can share a single QDQ pair as their
                inputs. If True, it will create identical and dedicated QDQ pair for each node.
            QDQOpTypePerChannelSupportToAxis = dictionary :
                Default is {}. Set channel axis for specific op type, for example: {'MatMul': 1}, and it's
                effective only when per channel quantization is supported and per_channel is True. If specific
                op type supports per channel quantization but not explicitly specified with channel axis,
                default channel axis will be used.
            CalibTensorRangeSymmetric = True/False :
                Default is False. If enabled, the final range of tensor during calibration will be explicitly
                set to symmetric to central point "0".
            CalibMovingAverage = True/False :
                Default is False. If enabled, the moving average of the minimum and maximum values will be
                computed when the calibration method selected is MinMax.
            CalibMovingAverageConstant = float :
                Default is 0.01. Constant smoothing factor to use when computing the moving average of the
                minimum and maximum values. Effective only when the calibration method selected is MinMax and
                when CalibMovingAverage is set to True.
            QuantizeBias = True/False :
                Default is True which quantizes floating-point biases and it solely inserts
                a DeQuantizeLinear node. If False, it remains floating-point bias and does not insert
                any quantization nodes associated with biases.
                This extra option is only effective when quant_format is QuantFormat.QDQ.
            SmoothQuant = True/False :
                Default is False. If enabled, SmoothQuant algorithm will be applied before quantization to do
                fake input channel quantization.
            SmoothQuantAlpha = float :
                Default is 0.5. It only works if SmoothQuant is True. It controls the difficulty of weight
                and activation quantization. A larger alpha value could be used on models with more significant
                activation outliers to migrate more quantization difficulty to weights.
            SmoothQuantFolding = True/False :
                Default is True. It only works if SmoothQuant is True. If enabled, inserted Mul ops during
                SmoothQuant will be folded into the previous op if the previous op is foldable.
            UseQDQContribOps = True/False :
                Default is False. If enabled, the inserted QuantizeLinear and DequantizeLinear ops will have the
                `com.microsoft` domain, which forces use of ONNX Runtime's QuantizeLinear and DequantizeLinear
                contrib op implementations. The contrib op implementations may support features not standardized
                into the ONNX specification (e.g., 16-bit quantization types).
            MinimumRealRange = float|None :
                Default is None. If set to a floating-point value, the calculation of the quantization parameters
                (i.e., scale and zero point) will enforce a minimum range between rmin and rmax. If (rmax-rmin)
                is less than the specified minimum range, rmax will be set to rmin + MinimumRealRange. This is
                necessary for EPs like QNN that require a minimum floating-point range when determining
                quantization parameters.
            TensorQuantOverrides = dictionary :
                Default is {}. Set tensor quantization overrides. The key is a tensor name and the value is a
                list of dictionaries. For per-tensor quantization, the list contains a single dictionary. For
                per-channel quantization, the list contains a dictionary for each channel in the tensor.
                Each dictionary contains optional overrides with the following keys and values.
                    'quant_type' = QuantType : The tensor's quantization data type.
                    'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.
                    'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.
                    'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also
                                               set `scale` or `zero_point`.
                    'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also
                                               set `scale` or `zero_point`.
                    'rmax' = Float           : Override the maximum real tensor value in calibration data.
                                               Invalid if also set `scale` or `zero_point`.
                    'rmin' = Float           : Override the minimum real tensor value in calibration data.
                                               Invalid if also set `scale` or `zero_point`.
            QDQKeepRemovableActivations = True/False:
                Default is False. If true, "removable" activations (e.g., Clip or Relu) will not be removed, and
                will be explicitly represented in the QDQ model. If false, these activations are automatically
                removed if activations are asymmetrically quantized. Keeping these activations is necessary if
                optimizations or EP transformations will later remove QuantizeLinear/DequantizeLinear
                operators from the model.
            QDQDisableWeightAdjustForInt32Bias = True/False:
                Default is False. If true, QDQ quantizer will not adjust the weight's scale when the bias
                has a scale (input_scale * weight_scale) that is too small.
    execution_provider : A enum indicates the Execution Provider such as: CPU, TRT, NNAPI, SNE, etc.
Raises:
    ValueError: Raise ValueError if execution provider is unknown
</pre> 
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="ae47023feec39104444adf5b125c82b3e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae47023feec39104444adf5b125c82b3e">&#9670;&nbsp;</a></span>calibrate_method</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.quantization.quantize.StaticQuantConfig.calibrate_method</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8227375afa0e4320d8384241b4bde833"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8227375afa0e4320d8384241b4bde833">&#9670;&nbsp;</a></span>calibration_data_reader</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.quantization.quantize.StaticQuantConfig.calibration_data_reader</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a97a9b31e9517cc6a9f6418a9d8caec0f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a97a9b31e9517cc6a9f6418a9d8caec0f">&#9670;&nbsp;</a></span>calibration_providers</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.quantization.quantize.StaticQuantConfig.calibration_providers</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a66161a0a298affd760e120417fb45f09"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66161a0a298affd760e120417fb45f09">&#9670;&nbsp;</a></span>extra_options</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.quantization.quantize.StaticQuantConfig.extra_options</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab5a24155d9eb0286f3ab66dc8a61a569"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5a24155d9eb0286f3ab66dc8a61a569">&#9670;&nbsp;</a></span>quant_format</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">onnxruntime.quantization.quantize.StaticQuantConfig.quant_format</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>chromadb_rest_wrapper/venv/lib/python3.10/site-packages/onnxruntime/quantization/<a class="el" href="chromadb__rest__wrapper_2venv_2lib_2python3_810_2site-packages_2onnxruntime_2quantization_2quantize_8py.html">quantize.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
